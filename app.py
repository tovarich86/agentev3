# -*- coding: utf-8 -*-
"""
AGENTE DE CONSULTA COM L√ìGICA ORIGINAL RESTAURADA (V5)
Aplica√ß√£o web para an√°lise de planos de incentivo de longo prazo, otimizada
para ser executada na Streamlit Community Cloud.

Esta vers√£o restaura a robustez e a intelig√™ncia de orquestra√ß√£o do agente
original, aplicando-as √† nova e eficiente estrutura de dados V7.
"""

import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import os
import re
import pandas as pd
import logging
import unicodedata
import requests

# --- CONFIGURA√á√ïES GERAIS ---
# O BASE_PATH agora aponta para uma pasta 'dados' relativa.
# Esta estrutura deve existir no seu reposit√≥rio do GitHub.
BASE_PATH = 'dados'

# Caminhos para os novos artefactos V7
FAISS_INDEX_PATH = os.path.join(BASE_PATH, 'faiss_index_contextual_v7.bin')
CHUNKS_MAP_PATH = os.path.join(BASE_PATH, 'chunks_com_metadata_contextual_v7.json')
CONSOLIDATED_TABLE_PATH = os.path.join(BASE_PATH, 'tabela_consolidada_v7.csv')

MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 20

# Configura√ß√µes da API do Gemini
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-1.5-flash-latest"

# Configura√ß√£o de logging
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- DICION√ÅRIOS DE CONHECIMENTO (do agente original) ---
TERMOS_TECNICOS_LTIP = {
    "A√ß√µes Restritas": ["Restricted Shares", "Plano de A√ß√µes Restritas", "Outorga de A√ß√µes", "a√ß√µes restritas", "RSU", "Restricted Stock Units"],
    "Op√ß√µes de Compra de A√ß√µes": ["Stock Options", "ESOP", "Plano de Op√ß√£o de Compra", "Outorga de Op√ß√µes", "op√ß√µes", "Plano de Op√ß√£o", "Plano de Op√ß√µes", "SOP"],
    "A√ß√µes Fantasmas": ["Phantom Shares", "A√ß√µes Virtuais"],
    "Op√ß√µes Fantasmas (SAR)": ["Phantom Options", "SAR", "Share Appreciation Rights", "Direito √† Valoriza√ß√£o de A√ß√µes"],
    "Planos com Condi√ß√£o de Performance": ["Performance Shares", "Performance Units", "PSU", "Plano de Desempenho", "Metas de Performance", "performance", "desempenho"],
    "Plano de Compra de A√ß√µes (ESPP)": ["Plano de Compra de A√ß√µes", "Employee Stock Purchase Plan", "ESPP", "A√ß√µes com Desconto"],
    "B√¥nus Diferido": ["Staying Bonus", "Retention Bonus", "B√¥nus de Perman√™ncia", "B√¥nus de Reten√ß√£o", "b√¥nus", "Deferred Bonus"],
    "Matching": ["Matching", "Contrapartida", "Co-investimento", "Plano de Matching", "investimento"],
    "Outorga": ["Outorga", "Concess√£o", "Grant", "Grant Date", "Data da Outorga", "Aprova√ß√£o"],
    "Vesting": ["Vesting", "Per√≠odo de Car√™ncia", "Condi√ß√µes de Car√™ncia", "Aquisi√ß√£o de Direitos", "car√™ncia", "cronograma de vesting"],
    "Antecipa√ß√£o de Vesting": ["Vesting Acelerado", "Accelerated Vesting", "Cl√°usula de Acelera√ß√£o", "antecipa√ß√£o de car√™ncia", "antecipa√ß√£o do vesting", "antecipa√ß√£o"],
    "Tranche / Lote": ["Tranche", "Lote", "Parcela do Vesting"],
    "Cliff": ["Cliff Period", "Per√≠odo de Cliff", "Car√™ncia Inicial"],
    "Pre√ßo": ["Pre√ßo", "Pre√ßo de Exerc√≠cio", "Strike", "Strike Price"],
    "Ciclo de Vida do Exerc√≠cio": ["Exerc√≠cio", "Per√≠odo de Exerc√≠cio", "pagamento", "liquida√ß√£o", "vencimento", "expira√ß√£o", "forma de liquida√ß√£o"],
    "Lockup": ["Lockup", "Per√≠odo de Lockup", "Restri√ß√£o de Venda", "per√≠odo de restri√ß√£o"],
    "Governan√ßa e Documentos": ["Regulamento", "Regulamento do Plano", "Contrato de Ades√£o", "Termo de Outorga", "Comit√™ de Remunera√ß√£o", "Comit√™ de Pessoas", "Delibera√ß√£o"],
    "Malus e Clawback": ["Malus", "Clawback", "Redu√ß√£o", "Devolu√ß√£o", "Cl√°usula de Recupera√ß√£o", "Forfeiture", "Cancelamento", "Perda do Direito"],
    "Estrutura do Plano/Programa": ["Plano", "Planos", "Programa", "Programas", "termos e condi√ß√µes gerais"],
    "Dilui√ß√£o": ["Dilui√ß√£o", "Dilution", "Capital Social"],
    "Eleg√≠veis": ["Participantes", "Benefici√°rios", "Eleg√≠veis", "Empregados", "Administradores", "Executivos", "Colaboradores", "Conselheiros"],
    "Condi√ß√£o de Sa√≠da": ["Desligamento", "Sa√≠da", "T√©rmino do Contrato", "Rescis√£o", "Demiss√£o", "Good Leaver", "Bad Leaver"],
    "Tratamento em Casos Especiais": ["Aposentadoria", "Morte", "Invalidez", "Reforma", "Afastamento"],
    "Indicadores": ["TSR", "Total Shareholder Return", "Retorno Total ao Acionista", "CDI", "IPCA", "Selic", "ROIC", "EBITDA", "LAIR", "Lucro", "CAGR", "Metas ESG", "Receita L√≠quida"],
    "Eventos Corporativos": ["IPO", "grupamento", "desdobramento", "cis√£o", "fus√£o", "incorpora√ß√£o", "bonifica√ß√µes", "bonifica√ß√£o"],
    "Mudan√ßa de Controle": ["Mudan√ßa de Controle", "Change of Control", "Evento de Liquidez"],
    "Dividendos": ["Dividendos", "Dividendo", "JCP", "Juros sobre capital pr√≥prio", "Tratamento de Dividendos", "dividend equivalent", "proventos"],
    "Encargos": ["Encargos", "Impostos", "Tributa√ß√£o", "Natureza Mercantil", "Natureza Remunerat√≥ria", "INSS", "IRRF"],
    "Contabilidade e Normas": ["IFRS 2", "CPC 10", "Valor Justo", "Fair Value", "Black-Scholes", "Despesa Cont√°bil", "Volatilidade"]
}
AVAILABLE_TOPICS = list(TERMOS_TECNICOS_LTIP.keys())

# --- CARREGAMENTO DE DADOS OTIMIZADO ---

@st.cache_resource
def load_all_artifacts():
    """
    Carrega todos os artefactos da nova estrutura de dados V7.
    """
    artifacts = {
        "model": None, "index": None, "chunks_dict": None, 
        "consolidated_df": None, "company_catalog": None
    }
    try:
        logger.info("A carregar o modelo de embedding...")
        artifacts["model"] = SentenceTransformer(MODEL_NAME)
        
        logger.info("A carregar o √≠ndice FAISS unificado...")
        artifacts["index"] = faiss.read_index(FAISS_INDEX_PATH)
        
        logger.info("A carregar o mapa de chunks com metadados...")
        with open(CHUNKS_MAP_PATH, 'r', encoding='utf-8') as f:
            chunks_data = json.load(f)
        artifacts["chunks_dict"] = {chunk['id']: chunk for chunk in chunks_data}
        
        logger.info("A carregar a tabela consolidada...")
        artifacts["consolidated_df"] = pd.read_csv(CONSOLIDATED_TABLE_PATH)

        try:
            from catalog_data import company_catalog_rich
            artifacts["company_catalog"] = company_catalog_rich
            logger.info("‚úÖ Cat√°logo de empresas carregado com sucesso.")
        except ImportError:
            logger.warning("`catalog_data.py` n√£o encontrado. A identifica√ß√£o de empresas por apelidos ser√° limitada.")
            artifacts["company_catalog"] = []

        logger.info("‚úÖ Todos os artefactos foram carregados com sucesso.")
        return artifacts
    except Exception as e:
        st.error(f"ERRO CR√çTICO AO CARREGAR ARTEFACTOS: {e}")
        st.error("Verifique se os ficheiros de √≠ndice e de dados (gerados pelo script de indexa√ß√£o V7) existem na pasta 'dados' do seu reposit√≥rio GitHub e n√£o est√£o corrompidos.")
        return artifacts


# --- L√ìGICA DE BUSCA, AN√ÅLISE E GERA√á√ÉO DE RESPOSTA ---

def create_analysis_plan_with_llm(query, company_catalog, all_known_companies):
    """
    Usa o Gemini para interpretar a pergunta e criar um plano de an√°lise robusto,
    identificando tanto empresas quanto t√≥picos.
    """
    if not GEMINI_API_KEY:
        st.error("Chave de API do Gemini n√£o configurada.")
        return None

    # 1. Usar LLM para identificar as empresas mencionadas na query
    #    Fornecemos a lista de empresas que temos nos dados como contexto para o LLM.
    company_prompt = f"""
    Dada a lista de empresas conhecidas: {json.dumps(all_known_companies)}.
    Analise a seguinte pergunta do utilizador: "{query}".
    Identifique TODAS as empresas da lista conhecida que s√£o mencionadas na pergunta.
    Se a pergunta mencionar um apelido (ex: Magalu), associe-o ao nome completo (ex: Magazine Luiza).
    Retorne APENAS uma lista JSON com os nomes can√≥nicos das empresas encontradas.
    Formato da resposta: ["Empresa A", "Empresa B"]
    """
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    payload = {"contents": [{"parts": [{"text": company_prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    
    mentioned_companies = []
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            mentioned_companies = json.loads(json_match.group(0))
    except Exception as e:
        logger.error(f"Falha ao chamar LLM para identificar empresas: {e}")
        # Se o LLM falhar, recorremos ao cat√°logo como fallback
        if company_catalog:
            companies_found_by_alias = {}
            for company_data in company_catalog:
                for alias in company_data.get("aliases", []):
                    if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query.lower()):
                        companies_found_by_alias[company_data["canonical_name"]] = len(alias.split())
            if companies_found_by_alias:
                mentioned_companies = [c for c, s in sorted(companies_found_by_alias.items(), key=lambda item: item[1], reverse=True)]

    if not mentioned_companies:
        return None # Nenhuma empresa encontrada

    # 2. Usar LLM para identificar os t√≥picos de interesse
    topic_prompt = f"""Voc√™ √© um consultor de ILP. Identifique os T√ìPICOS CENTRAIS da pergunta: "{query}".
    Retorne APENAS uma lista JSON com os t√≥picos mais relevantes da seguinte lista: {json.dumps(AVAILABLE_TOPICS)}.
    Se a pergunta for gen√©rica sobre uma empresa, selecione t√≥picos para uma an√°lise geral como ["Estrutura do Plano/Programa", "Vesting", "Eleg√≠veis"].
    Formato da resposta: ["T√≥pico 1", "T√≥pico 2"]"""
    
    payload = {"contents": [{"parts": [{"text": topic_prompt}]}]}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            topics = json.loads(json_match.group(0))
        else:
            topics = ["Estrutura do Plano/Programa", "Vesting", "Eleg√≠veis"] # Fallback
    except Exception as e:
        logger.error(f"Falha ao chamar LLM para t√≥picos: {e}")
        topics = ["Estrutura do Plano/Programa", "Vesting", "Eleg√≠veis"] # Fallback

    plan = {
        "empresas": mentioned_companies,
        "topicos": topics,
        "tipo_analise": "comparativa" if len(mentioned_companies) > 1 else "unica"
    }
    return plan


def execute_rag_analysis(plan, query, artifacts):
    """
    Executa o plano de an√°lise RAG, buscando e construindo o contexto.
    """
    model = artifacts["model"]
    index = artifacts["index"]
    chunks_dict = artifacts["chunks_dict"]
    
    all_context = ""
    all_sources = set()

    for company in plan['empresas']:
        logger.info(f"A executar a busca para a empresa: {company}")
        
        query_vector = model.encode([query], normalize_embeddings=True).astype('float32')
        distances, ids = index.search(query_vector, TOP_K_SEARCH)
        
        company_context = ""
        company_sources = set()
        
        if ids.size > 0:
            for chunk_id in ids[0]:
                if chunk_id != -1:
                    chunk_info = chunks_dict.get(chunk_id)
                    if chunk_info and company.lower() in chunk_info['metadata']['empresa'].lower():
                        if any(topic.lower() in (ct.lower() for ct in chunk_info['metadata']['chunk_topics']) for topic in plan['topicos']):
                            metadata = chunk_info['metadata']
                            company_context += f"--- Contexto (Fonte: {metadata['arquivo_origem']}) ---\n"
                            company_context += f"Sec√ß√£o: {metadata['section_title']}\n"
                            company_context += f"T√≥picos no Trecho: {', '.join(metadata['chunk_topics'])}\n"
                            company_context += f"Conte√∫do: {chunk_info['content']}\n\n"
                            company_sources.add(chunk_info['source'])

        if company_context:
            all_context += f"--- AN√ÅLISE PARA {company.upper()} ---\n\n{company_context}"
            all_sources.update(company_sources)

    return all_context, all_sources


def get_llm_response(prompt):
    """
    Gera a resposta final usando o contexto recuperado e a API do Gemini.
    """
    if not GEMINI_API_KEY:
        st.error("Chave de API do Gemini n√£o configurada.")
        return "ERRO: Chave de API n√£o configurada."

    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    payload = {"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"temperature": 0.2, "maxOutputTokens": 8192}}
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ERRO ao gerar resposta final com LLM: {e}")
        st.error(f"Falha na comunica√ß√£o com a API do Gemini: {e}")
        return f"ERRO ao gerar resposta final."


# --- INTERFACE STREAMLIT (Aplica√ß√£o Principal) ---
def main():
    st.set_page_config(page_title="Agente de An√°lise LTIP (V5)", page_icon="üîç", layout="wide")
    st.title("ü§ñ Agente de An√°lise H√≠brido e Comparativo (V5)")
    st.markdown("---")

    artifacts = load_all_artifacts()
    if not artifacts["model"]: return

    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        if artifacts["consolidated_df"] is not None:
            st.metric("Documentos na Tabela", len(artifacts["consolidated_df"]['caminho_completo'].unique()))
        if artifacts["chunks_dict"] is not None:
            st.metric("Total de Chunks Indexados", len(artifacts["chunks_dict"]))
        if artifacts["company_catalog"]:
            st.success("Cat√°logo de empresas carregado.")
        else:
            st.warning("Cat√°logo de empresas n√£o encontrado.")
        if artifacts["consolidated_df"] is not None:
            with st.expander("Empresas na Base de Dados"):
                st.dataframe(sorted(artifacts["consolidated_df"]['empresa'].unique()), use_container_width=True)
        st.success("‚úÖ Sistema pronto para an√°lise")

    st.header("üí¨ Fa√ßa a sua pergunta")
    st.info("**Exemplos:** `Como √© o plano de vesting da Vale?` ou `Compare o tratamento de dividendos da Petrobras com a Gerdau.`")
    user_query = st.text_area("Sua pergunta:", height=100, placeholder="Digite aqui...")

    if st.button("üîç Analisar", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            return

        with st.status("1Ô∏è‚É£ A criar plano de an√°lise...", expanded=True) as status:
            # Passa a lista de empresas conhecidas para a fun√ß√£o de cria√ß√£o do plano
            known_companies = artifacts["consolidated_df"]['empresa'].unique().tolist()
            plan = create_analysis_plan_with_llm(user_query, artifacts["company_catalog"], known_companies)
            
            if not plan:
                st.error("‚ùå Nenhuma empresa conhecida foi identificada na sua pergunta.")
                status.update(label="Falha ao criar plano.", state="error")
                return
            st.write(f"**Tipo de An√°lise:** {plan['tipo_analise'].title()}")
            st.write(f"**Empresa(s):** {', '.join(plan['empresas'])}")
            st.write(f"**T√≥picos Identificados:** {', '.join(plan['topicos'])}")
            status.update(label="Plano de an√°lise criado!", state="complete")

        with st.spinner("2Ô∏è‚É£ A executar a busca e a recolher o contexto..."):
            context, sources = execute_rag_analysis(plan, user_query, artifacts)
        
        st.markdown("---")
        st.subheader("üìã Resultado da An√°lise")

        if not context:
            st.warning("N√£o foram encontrados contextos relevantes para responder √† pergunta.")
            return
            
        with st.spinner("3Ô∏è‚É£ A gerar a resposta final com o Gemini..."):
            prompt = f"""Voc√™ √© um consultor especialista em planos de incentivo de longo prazo (ILP).
            Sua tarefa √© responder √† pergunta do utilizador com base no contexto fornecido.
            Se a pergunta for uma compara√ß√£o, crie um relat√≥rio comparativo bem estruturado.
            Se a pergunta for sobre uma √∫nica empresa, forne√ßa uma an√°lise detalhada.
            Seja profissional e baseie-se estritamente nos dados. Se a informa√ß√£o n√£o estiver no contexto, afirme isso claramente.

            PERGUNTA ORIGINAL DO UTILIZADOR: "{user_query}"

            CONTEXTO COLETADO DOS DOCUMENTOS:
            {context}

            RELAT√ìRIO ANAL√çTICO FINAL:
            """
            final_answer = get_llm_response(prompt)
            st.markdown(final_answer)

        if sources:
            with st.expander(f"üìö Documentos consultados ({len(sources)})", expanded=False):
                for source in sorted(list(sources)):
                    st.write(f"- {os.path.basename(source)}")

if __name__ == "__main__":
    main()
