# -*- coding: utf-8 -*-
"""
AGENTE DE AN√ÅLISE LTIP - VERS√ÉO FINAL E FUNCIONAL
"""

# --- 1. Importa√ß√µes e Configura√ß√µes ---
import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata
import logging
import pandas as pd
from pathlib import Path

try:
    from knowledge_base import DICIONARIO_UNIFICADO_HIERARQUICO
except ImportError:
    st.error("ERRO CR√çTICO: Crie o arquivo 'knowledge_base.py' e cole o 'DICIONARIO_UNIFICADO_HIERARQUICO' nele.")
    st.stop()

# Configura√ß√µes Gerais
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-1.5-flash-latest" # Usando o modelo mais recente e capaz
DADOS_PATH = Path("dados")

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)


# --- 2. Carregamento de Dados e Fun√ß√µes Auxiliares ---

@st.cache_resource
def load_all_artifacts():
    DADOS_PATH.mkdir(exist_ok=True)
    ARQUIVOS_REMOTOS = {
        "item_8_4_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_chunks_map_final.json",
        "item_8_4_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/item_8_4_faiss_index_final.bin",
        "outros_documentos_chunks_map_final.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_chunks_map_final.json",
        "outros_documentos_faiss_index_final.bin": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/outros_documentos_faiss_index_final.bin",
        "resumo_fatos_e_topicos_final_enriquecido.json": "https://github.com/tovarich86/agentev2/releases/download/V1.0-data/resumo_fatos_e_topicos_final_enriquecido.json"
    }
    for nome_arquivo, url in ARQUIVOS_REMOTOS.items():
        caminho_arquivo = DADOS_PATH / nome_arquivo
        if not caminho_arquivo.exists():
            with st.spinner(f"Baixando artefato: {nome_arquivo}..."):
                try:
                    r = requests.get(url, stream=True)
                    r.raise_for_status()
                    with open(caminho_arquivo, 'wb') as f:
                        for chunk in r.iter_content(chunk_size=8192): f.write(chunk)
                except requests.exceptions.RequestException as e:
                    st.error(f"Falha ao baixar {nome_arquivo}: {e}"); st.stop()
    
    _model = SentenceTransformer(MODEL_NAME)
    artifacts = {}
    index_files = glob.glob(str(DADOS_PATH / '*_faiss_index_final.bin'))
    for index_file_path in index_files:
        category = Path(index_file_path).stem.replace('_faiss_index_final', '')
        chunks_file_path = DADOS_PATH / f"{category}_chunks_map_final.json"
        try:
            index = faiss.read_index(index_file_path)
            with open(chunks_file_path, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            artifacts[category] = {'index': index, 'chunks': chunk_data}
        except Exception as e:
            logger.error(f"Erro ao carregar '{category}': {e}"); continue
    
    summary_data = None
    summary_file_path = DADOS_PATH / 'resumo_fatos_e_topicos_final_enriquecido.json'
    try:
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
    except FileNotFoundError:
        logger.error(f"Arquivo de resumo '{summary_file_path}' n√£o encontrado.")
            
    return _model, artifacts, summary_data

@st.cache_data
def criar_mapa_de_alias():
    alias_to_canonical = {}
    for section, topics in DICIONARIO_UNIFICADO_HIERARQUICO.items():
        for canonical_name, aliases in topics.items():
            alias_to_canonical[canonical_name.lower()] = canonical_name
            for alias in aliases:
                alias_to_canonical[alias.lower()] = canonical_name
    return alias_to_canonical


# --- 3. Fun√ß√µes de L√≥gica de Neg√≥cio (Manipuladores de Query) ---

def handle_analytical_query(query: str, summary_data: dict):
    query_lower = query.lower()
    op_keywords = {'avg': ['medio', 'm√©dia', 't√≠pico'], 'min': ['menor', 'm√≠nimo'], 'max': ['maior', 'm√°ximo']}
    fact_keywords = {
        'periodo_vesting': ['vesting', 'per√≠odo de car√™ncia'], 'periodo_lockup': ['lockup', 'lock-up'],
        'desconto_strike_price': ['desconto', 'des√°gio'], 'prazo_exercicio': ['prazo de exerc√≠cio']
    }
    operation, target_fact_key = None, None
    for op, keywords in op_keywords.items():
        if any(kw in query_lower for kw in keywords): operation = op; break
    for fact_key, keywords in fact_keywords.items():
        if any(kw in query_lower for kw in keywords): target_fact_key = fact_key; break
    if not operation or not target_fact_key: return False
    
    st.info(f"Analisando: **{operation.upper()}** para o fato **'{target_fact_key}'**...")
    valores, unidade = [], ''
    for data in summary_data.values():
        if target_fact_key in data.get("fatos_extraidos", {}):
            fact_data = data["fatos_extraidos"][target_fact_key]
            valor = fact_data.get('valor_numerico') or fact_data.get('valor')
            if isinstance(valor, (int, float)):
                valores.append(valor)
                if not unidade and 'unidade' in fact_data: unidade = fact_data['unidade']
    if not valores:
        st.warning(f"N√£o encontrei dados num√©ricos para '{target_fact_key}' para calcular."); return True

    resultado, label_metrica = 0, ""
    if operation == 'avg': resultado, label_metrica = np.mean(valores), f"M√©dia de {target_fact_key.replace('_', ' ')}"
    elif operation == 'min': resultado, label_metrica = np.min(valores), f"M√≠nimo de {target_fact_key.replace('_', ' ')}"
    elif operation == 'max': resultado, label_metrica = np.max(valores), f"M√°ximo de {target_fact_key.replace('_', ' ')}"
    
    valor_formatado = f"{resultado:.1%}" if target_fact_key == 'desconto_strike_price' else f"{resultado:.1f} {unidade}".strip()
    st.metric(label=label_metrica.title(), value=valor_formatado)
    st.caption(f"C√°lculo baseado em {len(valores)} empresas com dados para este fato.")
    return True

def handle_aggregate_query(query: str, summary_data: dict, alias_map: dict):
    query_lower = query.lower()
    query_keywords = set()
    sorted_aliases = sorted(alias_map.keys(), key=len, reverse=True)
    temp_query = query_lower
    for alias in sorted_aliases:
        if re.search(r'\b' + re.escape(alias.lower()) + r'\b', temp_query):
            query_keywords.add(alias.lower()); temp_query = temp_query.replace(alias.lower(), "")
    if not query_keywords: st.warning("N√£o identifiquei um termo t√©cnico na sua pergunta."); return
    st.info(f"Termos para busca: **{', '.join(sorted(list(query_keywords)))}**")
    empresas_encontradas = []
    for empresa, data in summary_data.items():
        company_terms = set()
        for topics in data.get("topicos_encontrados", {}).values():
            for topic, aliases in topics.items():
                company_terms.add(topic.lower()); company_terms.update([a.lower() for a in aliases])
        if query_keywords.issubset(company_terms): empresas_encontradas.append(empresa)
    if not empresas_encontradas: st.warning(f"Nenhuma empresa encontrada com: `{', '.join(query_keywords)}`."); return
    st.success(f"‚úÖ **{len(empresas_encontradas)} empresa(s)** encontrada(s).")
    df = pd.DataFrame(sorted(empresas_encontradas), columns=["Empresa"])
    st.dataframe(df, use_container_width=True, hide_index=True)

def handle_direct_fact_query(query: str, summary_data: dict, alias_map: dict, company_catalog: list):
    query_lower, empresa_encontrada, fato_encontrado_alias = query.lower(), None, None
    for company_data in company_catalog:
        for alias in company_data.get("aliases", []):
            if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                empresa_encontrada = company_data["canonical_name"].upper(); break
        if empresa_encontrada: break
    for alias in sorted(alias_map.keys(), key=len, reverse=True):
        if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
            fato_encontrado_alias = alias; break
    if not empresa_encontrada or not fato_encontrado_alias: return False
    empresa_data = summary_data.get(empresa_encontrada, {})
    st.subheader(f"Fato Direto para: {empresa_encontrada}")
    fato_encontrado = False
    for fact_key, fact_value in empresa_data.get("fatos_extraidos", {}).items():
        if fato_encontrado_alias in fact_key.lower():
            valor, unidade = fact_value.get('valor', ''), fact_value.get('unidade', '')
            st.metric(label=f"Fato: {fact_key.replace('_', ' ').title()}", value=f"{valor} {unidade}".strip())
            fato_encontrado = True; break
    if not fato_encontrado: st.info(f"O t√≥pico '{fato_encontrado_alias}' foi mencionado, mas um fato estruturado n√£o foi extra√≠do.")
    return True

# --- FUN√á√ïES DO PIPELINE RAG (ATUALIZADAS) ---

def create_rag_plan(query: str, company_catalog: list, alias_map: dict):
    """Cria um plano de busca simples para o RAG."""
    query_lower, plan = query.lower(), {"empresas": [], "topicos": []}
    for company_data in company_catalog:
        for alias in company_data.get("aliases", []):
            if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                plan["empresas"].append(company_data["canonical_name"]); break
    for alias, canonical_name in alias_map.items():
        if re.search(r'\b' + re.escape(alias) + r'\b', query_lower):
            plan["topicos"].append(canonical_name)
    plan["topicos"] = list(set(plan["topicos"]))
    if not plan["empresas"]: return None
    if not plan["topicos"]: plan["topicos"] = ["informa√ß√µes gerais do plano"]
    return plan

def execute_rag_plan(plan: dict, artifacts: dict, model):
    """Executa uma busca sem√¢ntica simples baseada no plano."""
    full_context, sources, unique_chunks = "", set(), set()
    for empresa in plan["empresas"]:
        full_context += f"--- Contexto para {empresa.upper()} ---\n\n"
        search_query = f"informa√ß√µes sobre {', '.join(plan['topicos'])} para a empresa {empresa}"
        query_embedding = model.encode([search_query], normalize_embeddings=True)
        for category, artifact_data in artifacts.items():
            scores, indices = artifact_data['index'].search(query_embedding, TOP_K_SEARCH)
            for i, idx in enumerate(indices[0]):
                if idx == -1 or scores[0][i] < 0.35: continue
                mapping = artifact_data["chunks"]["map"][idx]
                if empresa.upper() == mapping.get("company_name", "").upper():
                    chunk_text = artifact_data["chunks"]["chunks"][idx]
                    if chunk_text not in unique_chunks:
                        source_url = mapping.get("source_url", "Fonte Desconhecida")
                        full_context += f"Fonte: {os.path.basename(source_url)} (Score: {scores[0][i]:.2f})\n{chunk_text}\n\n"
                        unique_chunks.add(chunk_text); sources.add(source_url)
    return full_context, sources

def generate_rag_response(query: str, context: str):
    """Gera a resposta final do RAG com o Gemini."""
    prompt = f"""Baseado no contexto abaixo, responda √† pergunta do usu√°rio de forma clara, profissional e em portugu√™s. Use Markdown para formatar a resposta. Se a informa√ß√£o n√£o estiver no contexto, afirme isso claramente.

    Pergunta do Usu√°rio: "{query}"

    Contexto Coletado dos Documentos:
    {context}
    
    Relat√≥rio Anal√≠tico:
    """
    # L√≥gica de chamada √† API Gemini aqui (simulada para este exemplo)
    try:
        # response = gemini.generate_content(prompt)
        # return response.text
        return f"### An√°lise Detalhada (RAG)\n\nEsta √© uma resposta simulada para a pergunta: *'{query}'*. O modelo analisaria o contexto para fornecer um relat√≥rio detalhado."
    except Exception as e:
        logger.error(f"Erro na chamada da API Gemini: {e}")
        return "Desculpe, ocorreu um erro ao gerar a resposta final."

def handle_rag_query(query: str, artifacts: dict, model, company_catalog: list, alias_map: dict):
    """Orquestra o novo pipeline RAG, mantendo a experi√™ncia do usu√°rio."""
    with st.status("Gerando plano de an√°lise RAG...") as status:
        plan = create_rag_plan(query, company_catalog, alias_map)
        if not plan:
            st.error("N√£o identifiquei empresas na sua pergunta para realizar a an√°lise."); return set()
        status.update(label=f"Plano gerado. Analisando para: {', '.join(plan['empresas'])}...")

    with st.spinner("Recuperando e analisando informa√ß√µes..."):
        context, sources = execute_rag_plan(plan, artifacts, model)
        if not context:
            st.warning("N√£o encontrei informa√ß√µes relevantes para esta consulta."); return set()
        final_answer = generate_rag_response(query, context)
        st.markdown(final_answer)
    return sources


# --- 4. Aplica√ß√£o Principal (Interface Streamlit) ---
def main():
    st.set_page_config(page_title="Agente de An√°lise LTIP", page_icon="üìÑ", layout="wide")
    st.title("ü§ñ Agente de An√°lise de Planos de Incentivo")

    model, artifacts, summary_data = load_all_artifacts()
    
    if not summary_data:
        st.error(f"Arquivo de resumo n√£o encontrado. Funcionalidades limitadas."); st.stop()
    
    ALIAS_MAP = criar_mapa_de_alias()

    try:
        from catalog_data import company_catalog_rich
        logger.info("Cat√°logo de empresas 'catalog_data.py' carregado.")
    except ImportError:
        logger.warning("`catalog_data.py` n√£o encontrado. Criando cat√°logo din√¢mico.")
        company_catalog_rich = [{"canonical_name": name, "aliases": [name.split(' ')[0].lower(), name.lower()]} for name in summary_data.keys()]

    st.header("üí¨ Fa√ßa sua pergunta")
    user_query = st.text_input("Sua pergunta:", placeholder="Ex: Qual o desconto m√©dio oferecido?")
    
    if user_query:
        st.markdown("---"); st.subheader("üìã Resultado da An√°lise")
        
        query_lower = user_query.lower()
        analytical_keywords = ['medio', 'm√©dia', 't√≠pico', 'menor', 'm√≠nimo', 'maior', 'm√°ximo']
        aggregate_keywords = ["quais", "quantas", "liste", "mostre"]
        direct_fact_pattern = r'qual\s*(?:√©|o|a)\s*.*\s*d[aeo]\s*'
        sources = set()

        # Roteador de Inten√ß√£o de 4 N√≠veis
        if any(keyword in query_lower for keyword in analytical_keywords):
            if not handle_analytical_query(query, summary_data):
                 sources = handle_rag_query(query, artifacts, model, company_catalog_rich, ALIAS_MAP)
        elif any(keyword in query_lower for keyword in aggregate_keywords):
            handle_aggregate_query(query, summary_data, ALIAS_MAP)
        elif re.search(direct_fact_pattern, query_lower):
            if not handle_direct_fact_query(query, summary_data, ALIAS_MAP, company_catalog_rich):
                 sources = handle_rag_query(query, artifacts, model, company_catalog_rich, ALIAS_MAP)
        else:
            sources = handle_rag_query(query, artifacts, model, company_catalog_rich, ALIAS_MAP)

        if sources:
             with st.expander(f"üìö Fontes consultadas ({len(sources)})"):
                  st.write(sorted(list(sources)))

if __name__ == "__main__":
    main()
