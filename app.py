# app.py (vers√£o final com hyperlinks descritivos para as fontes)

import streamlit as st
import json
import numpy as np
import pandas as pd
import faiss
from sentence_transformers import SentenceTransformer
import requests
import re
import unicodedata
import logging
from pathlib import Path
import zipfile
import io
import shutil

# --- M√≥dulos do Projeto ---
from knowledge_base import DICIONARIO_UNIFICADO_HIERARQUICO
from analytical_engine import AnalyticalEngine

# --- Configura√ß√µes Gerais ---
st.set_page_config(page_title="Agente de An√°lise LTIP", page_icon="üîç", layout="wide", initial_sidebar_state="expanded")

MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
TOP_K_SEARCH = 7
GEMINI_API_KEY = st.secrets.get("GEMINI_API_KEY", "")
GEMINI_MODEL = "gemini-2.0-flash-lite"
GITHUB_SOURCE_URL = "https://github.com/tovarich86/agentev2/archive/refs/tags/V1.0-data.zip"
CACHE_DIR = Path("data_cache")
SUMMARY_FILENAME = "resumo_fatos_e_topicos_final_enriquecido.json"

logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')
logger = logging.getLogger(__name__)

# --- CARREGADOR DE DADOS ---
@st.cache_resource(show_spinner="Configurando o ambiente e baixando dados...")
def setup_and_load_data():
    # ... (c√≥digo da fun√ß√£o setup_and_load_data permanece o mesmo da vers√£o anterior) ...
    CACHE_DIR.mkdir(exist_ok=True)
    summary_file_path = CACHE_DIR / SUMMARY_FILENAME
    
    if not summary_file_path.exists():
        logger.info(f"Arquivo de resumo n√£o encontrado no cache. Baixando e preparando dados de {GITHUB_SOURCE_URL}...")
        if CACHE_DIR.exists():
            shutil.rmtree(CACHE_DIR)
        CACHE_DIR.mkdir(exist_ok=True)
        try:
            response = requests.get(GITHUB_SOURCE_URL, stream=True, timeout=60)
            response.raise_for_status() 
            with zipfile.ZipFile(io.BytesIO(response.content)) as z:
                z.extractall(CACHE_DIR)
            
            extracted_folder = next(CACHE_DIR.iterdir())
            if extracted_folder.is_dir():
                logger.info(f"Movendo conte√∫do de '{extracted_folder.name}' para a raiz do cache...")
                for item in extracted_folder.iterdir():
                    shutil.move(str(item), str(CACHE_DIR / item.name))
                extracted_folder.rmdir()
        except requests.exceptions.RequestException as e:
            st.error(f"Erro ao baixar os dados: {e}")
            st.stop()
    else:
        logger.info("Arquivos de dados encontrados no cache local.")

    model = SentenceTransformer(MODEL_NAME)
    artifacts = {}
    for index_file in CACHE_DIR.glob('*_faiss_index_final.bin'):
        category = index_file.stem.replace('_faiss_index_final', '')
        chunks_file = CACHE_DIR / f"{category}_chunks_map_final.json"
        try:
            artifacts[category] = {'index': faiss.read_index(str(index_file)), 'chunks': json.load(open(chunks_file, 'r', encoding='utf-8'))}
        except Exception as e:
            st.error(f"Falha ao carregar artefatos para a categoria '{category}': {e}")
            st.stop()
    try:
        with open(summary_file_path, 'r', encoding='utf-8') as f:
            summary_data = json.load(f)
    except FileNotFoundError:
        st.error(f"Erro cr√≠tico: '{SUMMARY_FILENAME}' n√£o foi encontrado ap√≥s a extra√ß√£o.")
        st.stop()
    return model, artifacts, summary_data

# --- FUN√á√ïES GLOBAIS (PRESERVADAS E ADAPTADAS) ---

def _create_flat_alias_map(kb: dict) -> dict:
    alias_to_canonical = {}
    for section, topics in kb.items():
        for topic_name_raw, aliases in topics.items():
            canonical_name = topic_name_raw.replace('_', ' ')
            alias_to_canonical[canonical_name.lower()] = canonical_name
            for alias in aliases:
                alias_to_canonical[alias.lower()] = canonical_name
    return alias_to_canonical

AVAILABLE_TOPICS = list(_create_flat_alias_map(DICIONARIO_UNIFICADO_HIERARQUICO).values())

def expand_search_terms(base_term: str, kb: dict) -> list[str]:
    # ... (c√≥digo da fun√ß√£o permanece o mesmo) ...
    base_term_lower = base_term.lower()
    expanded_terms = {base_term_lower}
    for section, topics in kb.items():
        for topic, aliases in topics.items():
            all_terms_in_group = {alias.lower() for alias in aliases} | {topic.lower().replace('_', ' ')}
            if base_term_lower in all_terms_in_group:
                expanded_terms.update(all_terms_in_group)
    return list(expanded_terms)


def search_by_tags(artifacts: dict, company_name: str, target_tags: list) -> list:
    # ... (c√≥digo da fun√ß√£o permanece o mesmo) ...
    results = []
    searchable_company_name = unicodedata.normalize('NFKD', company_name.lower()).encode('ascii', 'ignore').decode('utf-8').split(' ')[0]
    target_tags_lower = {tag.lower() for tag in target_tags}
    for index_name, artifact_data in artifacts.items():
        chunk_map = artifact_data.get('chunks', {}).get('map', [])
        all_chunks_text = artifact_data.get('chunks', {}).get('chunks', [])
        for i, mapping in enumerate(chunk_map):
            if searchable_company_name in mapping.get("company_name", "").lower():
                chunk_text = all_chunks_text[i]
                found_topics_in_chunk = re.findall(r'\[topico:([^\]]+)\]', chunk_text)
                if found_topics_in_chunk:
                    topics_in_chunk_set = {t.lower() for t in found_topics_in_chunk[0].split(',')}
                    intersection = target_tags_lower.intersection(topics_in_chunk_set)
                    if intersection:
                        results.append({'text': chunk_text, 'path': mapping.get('source_url', 'N/A'), 'index': i,'source': index_name, 'tag_found': ','.join(intersection), 'company': mapping.get("company_name")})
    return results

# --- L√ìGICA ROBUSTA (COM ATUALIZA√á√ïES) ---

def get_final_unified_answer(query: str, context: str) -> str:
    # ... (c√≥digo da fun√ß√£o permanece o mesmo) ...
    url = f"https://generativelanguage.googleapis.com/v1beta/models/{GEMINI_MODEL}:generateContent?key={GEMINI_API_KEY}"
    has_complete_8_4 = "formul√°rio de refer√™ncia" in query.lower() and "8.4" in query.lower()
    has_tagged_chunks = "--- CONTE√öDO RELEVANTE" in context
    structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
    if has_complete_8_4:
        structure_instruction = "ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4: Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia (a, b, c...)."
    elif has_tagged_chunks:
        structure_instruction = "PRIORIZE as informa√ß√µes dos chunks recuperados e organize a resposta de forma l√≥gica."
    prompt = f"""Voc√™ √© um consultor especialista em planos de incentivo de longo prazo (ILP).
    PERGUNTA ORIGINAL DO USU√ÅRIO: "{query}"
    CONTEXTO COLETADO DOS DOCUMENTOS:
    {context}
    {structure_instruction}
    INSTRU√á√ïES PARA O RELAT√ìRIO FINAL:
    1. Responda diretamente √† pergunta do usu√°rio com base no contexto fornecido.
    2. Seja detalhado, preciso e profissional na sua linguagem. Use formata√ß√£o Markdown.
    3. Se uma informa√ß√£o espec√≠fica pedida n√£o estiver no contexto, declare explicitamente: "Informa√ß√£o n√£o encontrada nas fontes analisadas.". N√£o invente dados.
    RELAT√ìRIO ANAL√çTICO FINAL:"""
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        logger.error(f"ERRO ao gerar resposta final com LLM: {e}")
        return f"Ocorreu um erro ao contatar o modelo de linguagem. Detalhes: {str(e)}"

# --- MUDAN√áA 1: execute_dynamic_plan agora retorna uma lista de dicion√°rios ---


def execute_dynamic_plan(plan: dict, artifacts: dict, model, kb: dict) -> tuple[str, list[dict]]:
    """
    Executa o plano de busca com uma estrat√©gia h√≠brida e um "Recuperador de √öltimo Recurso".

    Args:
        plan (dict): O plano de an√°lise contendo empresas e t√≥picos.
        artifacts (dict): Dicion√°rio com os √≠ndices FAISS e chunks.
        model: O modelo de embedding SentenceTransformer carregado.
        kb (dict): A base de conhecimento (DICIONARIO_UNIFICADO_HIERARQUICO).

    Returns:
        tuple[str, list[dict]]: Uma tupla contendo o contexto completo e uma lista
                                 de dicion√°rios com as fontes estruturadas.
    """
    full_context, unique_chunks_content = "", set()
    retrieved_sources_structured, seen_sources = [], set()

    class Config:
        MAX_CONTEXT_TOKENS, MAX_CHUNKS_PER_TOPIC, SCORE_THRESHOLD_GENERAL = 256000, 10, 0.4
        TOP_K_SEARCH = 7
    
    def add_unique_chunk_to_context(chunk_text: str, source_info_dict: dict):
        """Fun√ß√£o interna para adicionar chunks √∫nicos e estruturados ao contexto."""
        nonlocal full_context, unique_chunks_content, retrieved_sources_structured, seen_sources
        
        # Evita duplicatas de conte√∫do
        chunk_hash = hash(re.sub(r'\s+', '', chunk_text.lower())[:200])
        if chunk_hash in unique_chunks_content:
            return
        
        # (A l√≥gica de contagem de tokens seria inserida aqui se necess√°rio)

        unique_chunks_content.add(chunk_hash)
        
        # Limpa os metadados do texto antes de adicionar ao contexto do LLM
        clean_text = re.sub(r'\[(secao|topico):[^\]]+\]', '', chunk_text).strip()
        
        source_header = f"(Empresa: {source_info_dict['company']}, Documento: {source_info_dict['doc_type']})"
        full_context += f"--- CONTE√öDO RELEVANTE {source_header} ---\n{clean_text}\n\n"
        
        # Adiciona a fonte estruturada √† lista, evitando duplicatas de (empresa, url)
        source_tuple = (source_info_dict['company'], source_info_dict['url'])
        if source_tuple not in seen_sources:
            seen_sources.add(source_tuple)
            retrieved_sources_structured.append(source_info_dict)

    # --- ETAPA 1: BUSCA DE ALTA PRECIS√ÉO (Tags + Sem√¢ntica) ---
    for empresa in plan.get("empresas", []):
        logger.info(f"Executando busca de alta precis√£o para: {empresa}")
        
        # Expande todos os t√≥picos e seus aliases para as buscas
        target_tags = set()
        for topico in plan.get("topicos", []):
            target_tags.update(expand_search_terms(topico, kb))
        
        # 1a: Busca por Tags
        tagged_chunks = search_by_tags(artifacts, empresa, list(target_tags))
        for chunk_info in tagged_chunks:
            source_info = {
                'company': chunk_info['company'],
                'doc_type': chunk_info['source'],
                'url': chunk_info['path']
            }
            add_unique_chunk_to_context(chunk_info['text'], source_info)
        
        # 1b: Busca Sem√¢ntica
        for topico in plan.get("topicos", []):
            # Limita a 3 termos por t√≥pico para n√£o sobrecarregar
            for term in expand_search_terms(topico, kb)[:3]:
                search_query = f"informa√ß√µes sobre {term} no plano de remunera√ß√£o da empresa {empresa}"
                query_embedding = model.encode([search_query], normalize_embeddings=True)
                
                for doc_type, artifact_data in artifacts.items():
                    scores, indices = artifact_data['index'].search(query_embedding, Config.TOP_K_SEARCH)
                    for i, idx in enumerate(indices[0]):
                        if idx != -1 and scores[0][i] > Config.SCORE_THRESHOLD_GENERAL:
                            chunk_map_item = artifact_data['chunks']['map'][idx]
                            if empresa.lower() in chunk_map_item['company_name'].lower():
                                source_info = {
                                    'company': chunk_map_item['company_name'],
                                    'doc_type': doc_type,
                                    'url': chunk_map_item['source_url']
                                }
                                add_unique_chunk_to_context(artifact_data['chunks']['chunks'][idx], source_info)

    # --- ETAPA 2: RECUPERADOR DE √öLTIMO RECURSO ---
    if not full_context:
        logger.warning("Busca de alta precis√£o falhou. Ativando o Recuperador de √öltimo Recurso.")
        st.info("üí° A busca inicial n√£o retornou resultados de alta confian√ßa. Realizando uma varredura mais ampla...")
        
        for empresa in plan.get("empresas", []):
            expanded_terms = set()
            for topico in plan.get("topicos", []):
                expanded_terms.update(expand_search_terms(topico, kb))
            
            # Itera sobre todos os chunks da empresa em todos os artefatos
            for doc_type, artifact_data in artifacts.items():
                chunk_map = artifact_data.get('chunks', {}).get('map', [])
                all_chunks_text = artifact_data.get('chunks', {}).get('chunks', [])
                
                for i, mapping in enumerate(chunk_map):
                    if empresa.lower() in mapping.get("company_name", "").lower():
                        chunk_text = all_chunks_text[i]
                        
                        # Busca por qualquer um dos termos/aliases dentro do texto do chunk
                        for term in expanded_terms:
                            if re.search(r'\b' + re.escape(term) + r'\b', chunk_text, re.IGNORECASE):
                                source_info = {
                                    'company': mapping['company_name'],
                                    'doc_type': doc_type,
                                    'url': mapping['source_url']
                                }
                                add_unique_chunk_to_context(chunk_text, source_info)
                                break # Otimiza√ß√£o: vai para o pr√≥ximo chunk assim que encontrar um termo

    return full_context, retrieved_sources_structured

# --- Fim da Fun√ß√£o execute_dynamic_plan ---

def create_dynamic_analysis_plan(query, company_catalog_rich, kb, summary_data):
    # ... (c√≥digo da fun√ß√£o permanece o mesmo) ...
    query_lower = query.lower().strip()
    mentioned_companies = []
    if company_catalog_rich:
        companies_found_by_alias = {}
        for company_data in company_catalog_rich:
            for alias in company_data.get("aliases", []):
                if re.search(r'\b' + re.escape(alias.lower()) + r'\b', query_lower):
                    score = len(alias.split())
                    canonical_name = company_data["canonical_name"]
                    if canonical_name not in companies_found_by_alias or score > companies_found_by_alias[canonical_name]:
                        companies_found_by_alias[canonical_name] = score
        if companies_found_by_alias:
            mentioned_companies = [c for c, s in sorted(companies_found_by_alias.items(), key=lambda item: item[1], reverse=True)]
    if not mentioned_companies:
        for empresa_nome in summary_data.keys():
            if re.search(r'\b' + re.escape(empresa_nome.lower()) + r'\b', query_lower):
                mentioned_companies.append(empresa_nome)
    if not mentioned_companies: return {"status": "error", "plan": {}}
    alias_map = _create_flat_alias_map(kb)
    topics = list({canonical for alias, canonical in alias_map.items() if re.search(r'\b' + re.escape(alias) + r'\b', query_lower)})
    if not topics:
        logger.info("Nenhum t√≥pico local encontrado, consultando LLM para planejamento...")
        prompt = f"""...""" # Prompt do LLM
        try:
            llm_response = get_final_unified_answer(prompt, "")
            topics = json.loads(re.search(r'\[.*\]', llm_response, re.DOTALL).group())
        except Exception:
            topics = ["Estrutura do Plano", "Vesting", "Outorga"]
    plan = {"empresas": mentioned_companies, "topicos": topics}
    return {"status": "success", "plan": plan}


# --- MUDAN√áA 2: handle_rag_query agora manipula a lista de dicion√°rios de fontes ---
def handle_rag_query(query, artifacts, model, kb, company_catalog_rich, summary_data):
    # (A fun√ß√£o create_dynamic_analysis_plan permanece a mesma)
    
    with st.status("1Ô∏è‚É£ Gerando plano de an√°lise...", expanded=True) as status:
        plan_response = create_dynamic_analysis_plan(query, company_catalog_rich, kb, summary_data)
        if plan_response['status'] != "success" or not plan_response['plan']['empresas']:
            st.error("‚ùå N√£o consegui identificar empresas na sua pergunta.")
            return "An√°lise abortada.", []
        plan = plan_response['plan']
        st.write(f"**üè¢ Empresas identificadas:** {', '.join(plan['empresas'])}")
        st.write(f"**üìù T√≥picos a analisar:** {', '.join(plan['topicos'])}")
        status.update(label="‚úÖ Plano gerado com sucesso!", state="complete")
# --- L√ìGICA DE VERIFICA√á√ÉO E BUSCA FOR√áADA ---
    force_retrieve_flag = False
    empresa_alvo = plan['empresas'][0] # Foco na an√°lise de empresa √∫nica
    topicos_plano = {t.lower() for t in plan['topicos']}
    
    # Verifica no resumo se a empresa realmente tem o t√≥pico
    if empresa_alvo in summary_data:
        # Reconstroi os t√≥picos do resumo para verifica√ß√£o
        summary_topics = set()
        for section, topics_dict in summary_data[empresa_alvo].get("topicos_encontrados", {}).items():
            for topic_name, aliases in topics_dict.items():
                summary_topics.add(topic_name.lower().replace('_', ' '))
                for alias in aliases:
                    summary_topics.add(alias.lower())
        
        # Se algum t√≥pico do plano estiver no resumo, ativamos a busca for√ßada
        if not topicos_plano.isdisjoint(summary_topics):
            force_retrieve_flag = True
            st.info("üí° Detectado que a empresa possui men√ß√µes ao t√≥pico. Ativando busca profunda.")

    final_answer, all_sources_structured = "", []
    seen_sources_tuples = set()
    if len(plan['empresas']) > 1:
        # (L√≥gica de compara√ß√£o permanece a mesma, mas agora passa o 'force_retrieve_flag')
        # Omitida por brevidade, mas a ideia √© passar o flag para a chamada de execute_dynamic_plan
        pass # A l√≥gica completa deve ser mantida aqui
    else:
        with st.status("2Ô∏è‚É£ Recuperando contexto relevante...", expanded=True) as status:
            # Passa o novo flag para a fun√ß√£o de execu√ß√£o
            context, all_sources_structured = execute_dynamic_plan(plan, artifacts, model, kb, force_retrieve=force_retrieve_flag)
            if not context:
                st.error("‚ùå Mesmo com a busca aprofundada, n√£o encontrei detalhes suficientes nos documentos para a sua consulta.")
                return "Informa√ß√£o n√£o encontrada.", []
            st.write(f"**üìÑ Contexto recuperado de:** {len(all_sources_structured)} documento(s)")
            status.update(label="‚úÖ Contexto recuperado com sucesso!", state="complete")
        
        with st.status("3Ô∏è‚É£ Gerando resposta final...", expanded=True) as status:
            final_answer = get_final_unified_answer(query, context)
            status.update(label="‚úÖ An√°lise conclu√≠da!", state="complete")

    return final_answer, all_sources_structured


# --- FUN√á√ÉO PRINCIPAL DA APLICA√á√ÉO ---
def main():
    st.title("ü§ñ Agente de An√°lise de Planos de Incentivo (ILP)")
    st.markdown("---")

    model, artifacts, summary_data = setup_and_load_data()
    if not summary_data or not artifacts:
        st.error("‚ùå Falha cr√≠tica no carregamento dos dados.")
        st.stop()

    engine = AnalyticalEngine(summary_data, DICIONARIO_UNIFICADO_HIERARQUICO)
    try: from catalog_data import company_catalog_rich
    except ImportError: company_catalog_rich = []

    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        st.metric("Categorias de Documentos (RAG)", len(artifacts))
        st.metric("Empresas no Resumo", len(summary_data))
        with st.expander("Empresas com dados no resumo"):
            st.dataframe(sorted(list(summary_data.keys())), use_container_width=True, hide_index=False)
        st.success("‚úÖ Sistema pronto para an√°lise")
        st.info(f"Embedding Model: `{MODEL_NAME}`")
        st.info(f"Generative Model: `{GEMINI_MODEL}`")

    st.header("üí¨ Fa√ßa sua pergunta")
    # ... (UI com exemplos de perguntas) ...

    user_query = st.text_area("Sua pergunta:", height=100, placeholder="Ex: Compare o vesting da Vale e Movida")

    if st.button("üîç Analisar", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            st.stop()

        st.markdown("---"); st.subheader("üìã Resultado da An√°lise")
        
        query_lower = user_query.lower()
        aggregate_keywords = ["quais", "quantas", "liste", "qual a lista", "qual o desconto", "qual a m√©dia", "qual √© o"]

        if any(keyword in query_lower.split() for keyword in aggregate_keywords):
            with st.spinner("Analisando dados estruturados..."):
                report, dataframe = engine.answer_query(user_query)
                if report: st.markdown(report)
                if dataframe is not None: st.dataframe(dataframe, use_container_width=True, hide_index=True)
        else:
            final_answer, sources = handle_rag_query(user_query, artifacts, model, DICIONARIO_UNIFICADO_HIERARQUICO, company_catalog_rich, summary_data)
            st.markdown(final_answer)
            
            # --- MUDAN√áA 3: L√≥gica de exibi√ß√£o com hyperlinks ---
            if sources:
                with st.expander(f"üìö Documentos consultados ({len(sources)})"):
                    for src in sorted(sources, key=lambda x: x['company']):
                        display_text = f"{src['company']} - {src['doc_type'].replace('_', ' ')}"
                        st.markdown(f"- [{display_text}]({src['url']})")

if __name__ == "__main__":
    main()
