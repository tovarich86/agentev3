# -*- coding: utf-8 -*-
"""
AGENTE DE AN√ÅLISE LTIP - VERS√ÉO STREAMLIT
Aplica√ß√£o web para an√°lise de planos de incentivo de longo prazo
"""

import streamlit as st
import json
import time
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata
from catalog_data import company_catalog_rich

# --- CONFIGURA√á√ïES ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# --- DICION√ÅRIOS E LISTAS DE T√ìPICOS (sem altera√ß√µes) ---
TERMOS_TECNICOS_LTIP = {
    "tratamento de dividendos": ["tratamento de dividendos", "equivalente em dividendos", "dividendos", "juros sobre capital pr√≥prio", "proventos", "dividend equivalent", "dividendos pagos em a√ß√µes", "ajustes por dividendos"],
    "pre√ßo de exerc√≠cio": ["pre√ßo de exerc√≠cio", "strike price", "pre√ßo de compra", "pre√ßo fixo", "valor de exerc√≠cio", "pre√ßo pr√©-estabelecido", "pre√ßo de aquisi√ß√£o"],
    "forma de liquida√ß√£o": ["forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "entrega f√≠sica", "pagamento em dinheiro", "transfer√™ncia de a√ß√µes", "settlement"],
    "vesting": ["vesting", "per√≠odo de car√™ncia", "car√™ncia", "aquisi√ß√£o de direitos", "cronograma de vesting", "vesting schedule", "per√≠odo de cliff"],
    "eventos corporativos": ["eventos corporativos", "desdobramento", "grupamento", "dividendos pagos em a√ß√µes", "bonifica√ß√£o", "split", "ajustes", "reorganiza√ß√£o societ√°ria"],
    "stock options": ["stock options", "op√ß√µes de a√ß√µes", "op√ß√µes de compra", "SOP", "plano de op√ß√µes", "ESOP", "op√ß√£o de compra de a√ß√µes"],
    "a√ß√µes restritas": ["a√ß√µes restritas", "restricted shares", "RSU", "restricted stock units", "a√ß√µes com restri√ß√£o", "plano de a√ß√µes restritas"]
}
AVAILABLE_TOPICS = [
    "termos e condi√ß√µes gerais", "data de aprova√ß√£o e √≥rg√£o respons√°vel", "n√∫mero m√°ximo de a√ß√µes abrangidas", "n√∫mero m√°ximo de op√ß√µes a serem outorgadas", "condi√ß√µes de aquisi√ß√£o de a√ß√µes", "crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio", "pre√ßo de exerc√≠cio", "strike price", "crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio", "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "restri√ß√µes √† transfer√™ncia das a√ß√µes", "crit√©rios e eventos de suspens√£o/extin√ß√£o", "efeitos da sa√≠da do administrador", "Tipos de Planos", "Condi√ß√µes de Car√™ncia", "Vesting", "per√≠odo de car√™ncia", "cronograma de vesting", "Matching", "contrapartida", "co-investimento", "Lockup", "per√≠odo de lockup", "restri√ß√£o de venda", "Tratamento de Dividendos", "equivalente em dividendos", "proventos", "Stock Options", "op√ß√µes de a√ß√µes", "SOP", "A√ß√µes Restritas", "RSU", "restricted shares", "Eventos Corporativos", "IPO", "grupamento", "desdobramento"
]

# --- FUN√á√ïES AUXILIARES ---
def expand_search_terms(base_term):
    expanded_terms = [base_term.lower()]
    for category, terms in TERMOS_TECNICOS_LTIP.items():
        if any(term.lower() in base_term.lower() for term in terms):
            expanded_terms.extend([term.lower() for term in terms])
    return list(set(expanded_terms))

def search_by_tags(artifacts, company_name, target_tags):
    results = []
    for index_name, artifact_data in artifacts.items():
        chunk_data = artifact_data['chunks']
        for i, mapping in enumerate(chunk_data.get('map', [])):
            document_path = mapping['document_path']
            # Melhorando a busca para n√£o pegar apenas a primeira palavra
            if company_name.split(' ')[0].lower() in document_path.lower():
                chunk_text = chunk_data["chunks"][i]
                for tag in target_tags:
                    if f"T√≥picos:" in chunk_text and tag in chunk_text:
                        results.append({
                            'text': chunk_text, 'path': document_path, 'index': i,
                            'source': index_name, 'tag_found': tag
                        })
                        break
    return results

def normalize_name(name):
    try:
        nfkd_form = unicodedata.normalize('NFKD', name.lower())
        name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
        name = re.sub(r'[.,-]', '', name)
        suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
        for suffix in suffixes:
            name = re.sub(suffix, '', name, flags=re.IGNORECASE)
        return re.sub(r'\s+', ' ', name).strip() # Manter espa√ßos entre as palavras
    except Exception as e:
        return name.lower()

# --- CACHE PARA CARREGAR ARTEFATOS ---
@st.cache_resource
def load_all_artifacts():
    artifacts = {}
    model = SentenceTransformer(MODEL_NAME)
    dados_path = "dados"
    index_files = glob.glob(os.path.join(dados_path, '*_faiss_index.bin'))
    if not index_files:
        return None, None
    for index_file in index_files:
        category = os.path.basename(index_file).replace('_faiss_index.bin', '')
        chunks_file = os.path.join(dados_path, f"{category}_chunks_map.json")
        try:
            index = faiss.read_index(index_file)
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            artifacts[category] = {'index': index, 'chunks': chunk_data}
        except FileNotFoundError:
            continue
    if not artifacts:
        return None, None
    return artifacts, model

# --- FUN√á√ïES PRINCIPAIS ---
# NOVO: Esta √© a nova fun√ß√£o de an√°lise com l√≥gica de scoring.
def create_dynamic_analysis_plan_v2(query, company_catalog_rich, available_indices):
    """
    Gera um plano de a√ß√£o din√¢mico com identifica√ß√£o de empresas baseada em scoring
    para maior precis√£o, lidando com nomes compostos, apelidos e varia√ß√µes.
    """
    api_key = GEMINI_API_KEY
    # CORRE√á√ÉO: Atualizado o nome do modelo na URL para a vers√£o mais recente e est√°vel.
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}"

    # --- L√ìGICA DE IDENTIFICA√á√ÉO DE EMPRESAS POR SCORING ---
    query_lower = query.lower().strip()
    normalized_query_for_scoring = normalize_name(query_lower)
    company_scores = {}
    for company_data in company_catalog_rich:
        canonical_name = company_data["canonical_name"]
        score = 0
        for alias in company_data.get("aliases", []):
            if alias in query_lower:
                score += 10 * len(alias.split())
        name_for_parts = normalize_name(canonical_name)
        parts = name_for_parts.split()
        for part in parts:
            if len(part) > 2 and part in normalized_query_for_scoring.split():
                score += 1
        if score > 0:
            company_scores[canonical_name] = score
    mentioned_companies = []
    if company_scores:
        sorted_companies = sorted(company_scores.items(), key=lambda item: item[1], reverse=True)
        max_score = sorted_companies[0][1]
        if max_score > 0:
            mentioned_companies = [company for company, score in sorted_companies if score >= max_score * 0.7]

    # --- FIM DA L√ìGICA DE IDENTIFICA√á√ÉO ---
    prompt = f'Voc√™ √© um planejador de an√°lise. Sua tarefa √© analisar a "Pergunta do Usu√°rio" e identificar os t√≥picos de interesse. Instru√ß√µes: 1. Identifique os T√≥picos: Analise a pergunta para identificar os t√≥picos de interesse. Se a pergunta for gen√©rica (ex: "resumo dos planos", "an√°lise da empresa"), inclua todos os "T√≥picos de An√°lise Dispon√≠veis". Se for espec√≠fica (ex: "fale sobre o vesting e dividendos"), inclua apenas os t√≥picos relevantes. 2. Formate a Sa√≠da: Retorne APENAS uma lista JSON de strings contendo os t√≥picos identificados. T√≥picos de An√°lise Dispon√≠veis: {json.dumps(AVAILABLE_TOPICS, indent=2)} Pergunta do Usu√°rio: "{query}" T√≥picos de Interesse (responda APENAS com a lista JSON de strings):'
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        topics = json.loads(json_match.group(0)) if json_match else AVAILABLE_TOPICS
    except Exception:
        topics = AVAILABLE_TOPICS
    plan = {"empresas": mentioned_companies, "topicos": topics}
    return {"status": "success", "plan": plan}


# Adicione esta constante no topo do seu script para controle
MAX_CHUNKS_PER_TOPIC = 3 # Limite de chunks por t√≥pico, para controle de token

# MANTENDO A FUN√á√ÉO DE EXECU√á√ÉO ORIGINAL, AGORA COM DE-DUPLICA√á√ÉO
def execute_dynamic_plan(plan, query_intent, artifacts, model):
    full_context = ""
    all_retrieved_docs = set()
    
    # --- L√ìGICA DE DEDUPLICA√á√ÉO ---
    # Usamos um set para armazenar o conte√∫do dos chunks e evitar duplicatas exatas.
    unique_chunks_content = set()

    def add_unique_chunk_to_context(chunk_text, source_info):
        """Fun√ß√£o auxiliar para adicionar chunks ao contexto, evitando duplicatas."""
        nonlocal full_context # Permite modificar a vari√°vel do escopo externo
        
        # Usamos uma vers√£o simplificada do texto como chave para o set
        chunk_key = re.sub(r'\s+', '', chunk_text).lower()

        if chunk_key not in unique_chunks_content:
            unique_chunks_content.add(chunk_key)
            full_context += f"--- {source_info} ---\n{chunk_text}\n\n"
            return True
        return False

    if query_intent == 'item_8_4_query':
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            if 'item_8_4' in artifacts:
                artifact_data = artifacts['item_8_4']
                chunk_data = artifact_data['chunks']
                
                context_added = False
                for i, mapping in enumerate(chunk_data.get('map', [])):
                    document_path = mapping['document_path']
                    if empresa.split(' ')[0].lower() in document_path.lower():
                        chunk_text = chunk_data["chunks"][i]
                        all_retrieved_docs.add(str(document_path))
                        # Tenta adicionar o chunk e verifica se foi bem-sucedido (se n√£o era duplicata)
                        if add_unique_chunk_to_context(chunk_text, f"Chunk Item 8.4 (Doc: {mapping['document_path']})"):
                            context_added = True
                
                # Adiciona os cabe√ßalhos apenas se algum conte√∫do foi realmente adicionado
                if context_added:
                    full_context = f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n=== SE√á√ÉO COMPLETA DO ITEM 8.4 - {empresa.upper()} ===\n\n" + full_context.split(f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n")[1]
                    full_context += f"=== FIM DA SE√á√ÉO ITEM 8.4 - {empresa.upper()} ===\n\n"

            # A busca complementar pode continuar aqui...
            full_context += f"--- FIM DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
    else: # Busca geral
        for empresa in plan.get("empresas", []):
            full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
            
            # Busca por tags com de-duplica√ß√£o
            target_tags = list(set(term for topico in plan.get("topicos", []) for term in expand_search_terms(topico)))
            tagged_chunks = search_by_tags(artifacts, empresa, [tag.title() for tag in target_tags if len(tag) > 3])
            
            # Adiciona os chunks com tags primeiro, pois s√£o de alta relev√¢ncia
            if tagged_chunks:
                full_context += f"=== CHUNKS COM TAGS ESPEC√çFICAS - {empresa.upper()} ===\n\n"
                for chunk_info in tagged_chunks:
                    add_unique_chunk_to_context(chunk_info['text'], f"Chunk com tag '{chunk_info['tag_found']}' (Doc: {chunk_info['path']})")
                full_context += f"=== FIM DOS CHUNKS COM TAGS - {empresa.upper()} ===\n\n"
            
            # Busca sem√¢ntica complementar (ainda n√£o implementada no seu c√≥digo, mas a l√≥gica de de-duplica√ß√£o se aplicaria aqui tamb√©m)
            # ...
            
            full_context += f"--- FIM DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"

    if not unique_chunks_content:
        return "Nenhuma informa√ß√£o encontrada para os crit√©rios especificados.", []
        
    return full_context, [str(doc) for doc in all_retrieved_docs]

# MANTENDO A FUN√á√ÉO DE GERA√á√ÉO DE RESPOSTA ORIGINAL
# MANTENDO A FUN√á√ÉO DE GERA√á√ÉO DE RESPOSTA ORIGINAL
def get_final_unified_answer(query, context):
    """Gera a resposta final usando o contexto recuperado."""
    # CORRE√á√ÉO: Atualizado o nome do modelo na URL para a vers√£o mais recente e est√°vel.
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={GEMINI_API_KEY}"
    
    has_complete_8_4 = "=== SE√á√ÉO COMPLETA DO ITEM 8.4" in context
    has_tagged_chunks = "=== CHUNKS COM TAGS ESPEC√çFICAS" in context
    
    if has_complete_8_4:
        structure_instruction = """
**ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4:**
Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia:
a) Termos e condi√ß√µes gerais dos planos
b) Data de aprova√ß√£o e √≥rg√£o respons√°vel
c) N√∫mero m√°ximo de a√ß√µes abrangidas pelos planos
d) N√∫mero m√°ximo de op√ß√µes a serem outorgadas
e) Condi√ß√µes de aquisi√ß√£o de a√ß√µes
f) Crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio
g) Crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio
h) Forma de liquida√ß√£o
i) Restri√ß√µes √† transfer√™ncia das a√ß√µes
j) Crit√©rios e eventos que, quando verificados, ocasionar√£o a suspens√£o, altera√ß√£o ou extin√ß√£o do plano
k) Efeitos da sa√≠da do administrador do cargo na manuten√ß√£o dos seus direitos no plano

Para cada subitem, extraia e organize as informa√ß√µes encontradas na SE√á√ÉO COMPLETA DO ITEM 8.4.
"""
    elif has_tagged_chunks:
        structure_instruction = "**PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS e organize a resposta de forma l√≥gica usando Markdown."
    else:
        structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
        
    prompt = f'Voc√™ √© um analista financeiro s√™nior especializado em Formul√°rios de Refer√™ncia da CVM. PERGUNTA ORIGINAL DO USU√ÅRIO: "{query}" CONTEXTO COLETADO DOS DOCUMENTOS: {context} {structure_instruction} INSTRU√á√ïES PARA O RELAT√ìRIO FINAL: 1. Responda diretamente √† pergunta do usu√°rio. 2. PRIORIZE informa√ß√µes da SE√á√ÉO COMPLETA DO ITEM 8.4 ou de CHUNKS COM TAGS ESPEC√çFICAS quando dispon√≠veis. 3. Use informa√ß√µes complementares apenas para esclarecer. 4. Seja detalhado, preciso e profissional. 5. Se alguma informa√ß√£o n√£o estiver dispon√≠vel, indique: "Informa√ß√£o n√£o encontrada nas fontes analisadas". RELAT√ìRIO ANAL√çTICO FINAL:'
    
    payload = {"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}}
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        return f"ERRO ao gerar resposta final: {e}"
        

# --- INTERFACE STREAMLIT ---
def main():
    st.set_page_config(page_title="Agente de An√°lise LTIP", page_icon="üîç", layout="wide", initial_sidebar_state="expanded")
    st.title("ü§ñ Agente de An√°lise de Planos de Incentivo Longo Prazo ILP")
    st.markdown("---")
    
    with st.spinner("Inicializando sistema..."):
        loaded_artifacts, embedding_model = load_all_artifacts()
    
    if not loaded_artifacts:
        st.error("‚ùå Erro no carregamento dos artefatos. Verifique os arquivos na pasta 'dados'.")
        return
    
    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        st.metric("Fontes dispon√≠veis", len(loaded_artifacts))
        st.metric("Empresas identificadas", len(company_catalog_rich))
        with st.expander("üìã Ver empresas dispon√≠veis"):
            sorted_companies = sorted([company['canonical_name'] for company in company_catalog_rich])
            for company_name in sorted_companies:
                st.write(f"‚Ä¢ {company_name}")
        st.success("‚úÖ Sistema carregado")
        st.info(f"Modelo: {MODEL_NAME}")

    st.header("üí¨ Fa√ßa sua pergunta")
    
    # CORRE√á√ÉO: O expander agora cont√©m apenas o texto informativo.
    with st.expander("üí° Entenda como funciona e veja dicas para perguntas ideais"):
        st.markdown("""
**Este agente analisa Planos de Incentivo de Longo Prazo (ILPs) usando documentos p√∫blicos das empresas listadas.**

###  Formatos de Pergunta Recomendados

**1. Perguntas Espec√≠ficas** *(formato ideal)*  
Combine t√≥picos + empresas para an√°lises direcionadas:
- *"Qual a liquida√ß√£o e dividendos da **Vale**?"*
- *"Vesting da **Petrobras**"* 
- *"Ajustes de pre√ßo da **Ambev**"*
- *"Per√≠odo de lockup da **Magalu**"*
- *"Condi√ß√µes de car√™ncia **YDUQS**"*

**2.  Vis√£o Geral (Item 8.4)**  
Solicite a se√ß√£o completa do Formul√°rio de Refer√™ncia:
- *"Item 8.4 da **Vibra**"*
- *"Resumo 8.4 da **Raia Drogasil**"*
- *"Formul√°rio completo da **WEG**"*

**3.  An√°lise Comparativa**  
Compare caracter√≠sticas entre empresas:
- *"Liquida√ß√£o **Localiza** vs **Movida**"*
- *"Dividendos **Eletrobras** vs **Energisa**"*
""")

    # CORRE√á√ÉO: O campo de texto e o bot√£o agora est√£o fora do expander.
    user_query = st.text_area("Digite sua pergunta:", height=100, placeholder="Ex: Fale sobre o vesting da Magalu ou planos da Vibra Energia")
    
    if st.button("üîç Analisar", type="primary", use_container_width=True):
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            return
        
        # O resto da sua l√≥gica de an√°lise continua aqui...
        with st.container():
            st.markdown("---")
            st.subheader("üìã Processo de An√°lise")
            
            with st.status("1Ô∏è‚É£ Gerando plano de an√°lise...", expanded=True) as status:
                plan_response = create_dynamic_analysis_plan_v2(user_query, company_catalog_rich, list(loaded_artifacts.keys()))
                plan = plan_response['plan']
                if plan.get('empresas'):
                    st.write(f"**üè¢ Empresas identificadas:** {', '.join(plan.get('empresas', []))}")
                else:
                    st.write("**üè¢ Empresas identificadas:** Nenhuma")
                st.write(f"**üìù T√≥picos a analisar:** {len(plan.get('topicos', []))}")
                status.update(label="‚úÖ Plano gerado com sucesso!", state="complete")

            if not plan.get("empresas"):
                st.error("‚ùå N√£o consegui identificar empresas na sua pergunta. Tente usar nomes, apelidos ou marcas conhecidas (ex: Magalu, Vivo, Ita√∫).")
                return

            with st.status("2Ô∏è‚É£ Recuperando contexto relevante...", expanded=True) as status:
                query_intent = 'item_8_4_query' if any(term in user_query.lower() for term in ['8.4', '8-4', 'item 8.4', 'formul√°rio']) else 'general_query'
                st.write(f"**üéØ Estrat√©gia detectada:** {'Item 8.4 completo' if query_intent == 'item_8_4_query' else 'Busca geral'}")
                retrieved_context, sources = execute_dynamic_plan(plan, query_intent, loaded_artifacts, embedding_model)
                if not retrieved_context.strip() or "Nenhuma informa√ß√£o encontrada" in retrieved_context:
                    st.error("‚ùå N√£o encontrei informa√ß√µes relevantes nos documentos para a sua consulta.")
                    return
                st.write(f"**üìÑ Contexto recuperado de:** {len(set(sources))} documento(s)")
                status.update(label="‚úÖ Contexto recuperado com sucesso!", state="complete")
            
            with st.status("3Ô∏è‚É£ Gerando resposta final...", expanded=True) as status:
                final_answer = get_final_unified_answer(user_query, retrieved_context)
                status.update(label="‚úÖ An√°lise conclu√≠da!", state="complete")
            
            st.markdown("---")
            st.subheader("üìÑ Resultado da An√°lise")
            with st.container():
                st.markdown(final_answer)

if __name__ == "__main__":
    main()
