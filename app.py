# -*- coding: utf-8 -*-
"""
AGENTE DE AN√ÅLISE LTIP - VERS√ÉO STREAMLIT
Aplica√ß√£o web para an√°lise de planos de incentivo de longo prazo
"""

import streamlit as st
import json
import time
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata

# --- CONFIGURA√á√ïES ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
AMBIGUITY_THRESHOLD = 3

# Configura√ß√£o da API Key do Gemini
GEMINI_API_KEY = st.secrets["GEMINI_API_KEY"]

# Dicion√°rio especializado para termos t√©cnicos de LTIP
TERMOS_TECNICOS_LTIP = {
    "tratamento de dividendos": [
        "tratamento de dividendos", "equivalente em dividendos", "dividendos", 
        "juros sobre capital pr√≥prio", "proventos", "dividend equivalent",
        "dividendos pagos em a√ß√µes", "ajustes por dividendos"
    ],
    "pre√ßo de exerc√≠cio": [
        "pre√ßo de exerc√≠cio", "strike price", "pre√ßo de compra", "pre√ßo fixo", 
        "valor de exerc√≠cio", "pre√ßo pr√©-estabelecido", "pre√ßo de aquisi√ß√£o"
    ],
    "forma de liquida√ß√£o": [
        "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "entrega f√≠sica", 
        "pagamento em dinheiro", "transfer√™ncia de a√ß√µes", "settlement"
    ],
    "vesting": [
        "vesting", "per√≠odo de car√™ncia", "car√™ncia", "aquisi√ß√£o de direitos", 
        "cronograma de vesting", "vesting schedule", "per√≠odo de cliff"
    ],
    "eventos corporativos": [
        "eventos corporativos", "desdobramento", "grupamento", "dividendos pagos em a√ß√µes",
        "bonifica√ß√£o", "split", "ajustes", "reorganiza√ß√£o societ√°ria"
    ],
    "stock options": [
        "stock options", "op√ß√µes de a√ß√µes", "op√ß√µes de compra", "SOP", 
        "plano de op√ß√µes", "ESOP", "op√ß√£o de compra de a√ß√µes"
    ],
    "a√ß√µes restritas": [
        "a√ß√µes restritas", "restricted shares", "RSU", "restricted stock units", 
        "a√ß√µes com restri√ß√£o", "plano de a√ß√µes restritas"
    ]
}

# T√≥picos expandidos de an√°lise
AVAILABLE_TOPICS = [
    "termos e condi√ß√µes gerais", "data de aprova√ß√£o e √≥rg√£o respons√°vel",
    "n√∫mero m√°ximo de a√ß√µes abrangidas", "n√∫mero m√°ximo de op√ß√µes a serem outorgadas",
    "condi√ß√µes de aquisi√ß√£o de a√ß√µes", "crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio",
    "pre√ßo de exerc√≠cio", "strike price",
    "crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio", 
    "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento",
    "restri√ß√µes √† transfer√™ncia das a√ß√µes", "crit√©rios e eventos de suspens√£o/extin√ß√£o",
    "efeitos da sa√≠da do administrador", "Tipos de Planos", "Condi√ß√µes de Car√™ncia", 
    "Vesting", "per√≠odo de car√™ncia", "cronograma de vesting",
    "Matching", "contrapartida", "co-investimento",
    "Lockup", "per√≠odo de lockup", "restri√ß√£o de venda",
    "Tratamento de Dividendos", "equivalente em dividendos", "proventos",
    "Stock Options", "op√ß√µes de a√ß√µes", "SOP",
    "A√ß√µes Restritas", "RSU", "restricted shares",
    "Eventos Corporativos", "IPO", "grupamento", "desdobramento"
]

# --- FUN√á√ïES AUXILIARES ---

def expand_search_terms(base_term):
    """Expande um termo base com sin√¥nimos e varia√ß√µes t√©cnicas."""
    expanded_terms = [base_term.lower()]
    
    for category, terms in TERMOS_TECNICOS_LTIP.items():
        if any(term.lower() in base_term.lower() for term in terms):
            expanded_terms.extend([term.lower() for term in terms])
    
    return list(set(expanded_terms))

def search_by_tags(artifacts, company_name, target_tags):
    """Busca chunks que contenham tags espec√≠ficas para uma empresa."""
    results = []
    
    for index_name, artifact_data in artifacts.items():
        chunk_data = artifact_data['chunks']
        
        for i, mapping in enumerate(chunk_data.get('map', [])):
            document_path = mapping['document_path']
            if re.search(re.escape(company_name.split(' ')[0]), document_path, re.IGNORECASE):
                chunk_text = chunk_data["chunks"][i]
                
                # Verifica se o chunk cont√©m as tags procuradas
                for tag in target_tags:
                    if f"T√≥picos:" in chunk_text and tag in chunk_text:
                        results.append({
                            'text': chunk_text,
                            'path': document_path,
                            'index': i,
                            'source': index_name,
                            'tag_found': tag
                        })
                        break
    
    return results

def normalize_name(name):
    """Normaliza nomes de empresas para compara√ß√£o."""
    try:
        nfkd_form = unicodedata.normalize('NFKD', name.lower())
        name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
        name = re.sub(r'[.,-]', '', name)
        suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
        for suffix in suffixes:
            name = re.sub(suffix, '', name, flags=re.IGNORECASE)
        return re.sub(r'\s+', '', name).strip()
    except Exception as e:
        return name.lower()

# --- CACHE PARA CARREGAR ARTEFATOS ---

@st.cache_resource
def load_all_artifacts():
    """Carrega todos os artefatos e constr√≥i um cat√°logo de nomes de empresas can√¥nicos."""
    artifacts = {}
    canonical_company_names = set()
    
    # Carrega modelo de embedding
    model = SentenceTransformer(MODEL_NAME)
    
    # Busca arquivos na pasta dados
    dados_path = "dados"
    index_files = glob.glob(os.path.join(dados_path, '*_faiss_index.bin'))
    
    if not index_files:
        return None, None, None

    for index_file in index_files:
        category = os.path.basename(index_file).replace('_faiss_index.bin', '')
        chunks_file = os.path.join(dados_path, f"{category}_chunks_map.json")
        
        try:
            index = faiss.read_index(index_file)
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            artifacts[category] = {'index': index, 'chunks': chunk_data}
            
            for mapping in chunk_data.get('map', []):
                company_name = mapping['document_path'].split('/')[0]
                canonical_company_names.add(company_name)
                
        except FileNotFoundError:
            continue
    
    if not artifacts:
        return None, None, None

    return artifacts, model, list(canonical_company_names)

# --- FUN√á√ïES PRINCIPAIS ---

def create_dynamic_analysis_plan(query, company_catalog, available_indices):
    """
    Gera um plano de a√ß√£o din√¢mico em JSON com identifica√ß√£o robusta de empresas,
    incluindo siglas e nomes curtos.
    """
    api_key = GEMINI_API_KEY
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={api_key}"
    
    # --- L√ìGICA DE IDENTIFICA√á√ÉO DE EMPRESAS APRIMORADA (VERS√ÉO 2) ---
    
    mentioned_companies = set()
    query_lower = query.lower().strip()

    # 1. Pr√©-processamento: Criar um mapa de busca de empresas.
    company_search_map = {}
    for canonical_name in company_catalog:
        # Adiciona o nome completo normalizado
        normalized_full_name = normalize_name(canonical_name)
        if normalized_full_name not in company_search_map:
            company_search_map[normalized_full_name] = []
        company_search_map[normalized_full_name].append(canonical_name)

        # Adiciona partes significativas do nome (incluindo siglas)
        name_for_parts = re.sub(r'[.,()]', '', canonical_name)
        suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
        for suffix in suffixes:
            name_for_parts = re.sub(suffix, '', name_for_parts, flags=re.IGNORECASE)
        
        parts = name_for_parts.split()
        for part in parts:
            # CORRE√á√ÉO: Alterado de len(part) > 3 para len(part) >= 2 para incluir B3, CCR, etc.
            if len(part) >= 2:
                key = normalize_name(part)
                if key not in company_search_map:
                    company_search_map[key] = []
                if canonical_name not in company_search_map[key]:
                    company_search_map[key].append(canonical_name)

    # 2. Busca: Tokenizar a consulta do usu√°rio e verificar cada token no mapa.
    query_tokens = re.split(r'[\s,.-]+', query_lower)
    for token in query_tokens:
        normalized_token = normalize_name(token)
        if normalized_token in company_search_map:
            for company_name in company_search_map[normalized_token]:
                mentioned_companies.add(company_name)

    # --- FIM DA L√ìGICA DE IDENTIFICA√á√ÉO ---

    prompt = f"""
Voc√™ √© um planejador de an√°lise. Sua tarefa √© analisar a "Pergunta do Usu√°rio" e identificar os t√≥picos de interesse.

**Instru√ß√µes:**
1. **Identifique os T√≥picos:** Analise a pergunta para identificar os t√≥picos de interesse. Se a pergunta for gen√©rica (ex: "resumo dos planos", "an√°lise da empresa"), inclua todos os "T√≥picos de An√°lise Dispon√≠veis". Se for espec√≠fica (ex: "fale sobre o vesting e dividendos"), inclua apenas os t√≥picos relevantes.
2. **Formate a Sa√≠da:** Retorne APENAS uma lista JSON de strings contendo os t√≥picos identificados.

**T√≥picos de An√°lise Dispon√≠veis:** {json.dumps(AVAILABLE_TOPICS, indent=2)}

**Pergunta do Usu√°rio:** "{query}"

**T√≥picos de Interesse (responda APENAS com a lista JSON de strings):**
"""
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            topics = json.loads(json_match.group(0))
            plan = {"empresas": list(mentioned_companies), "topicos": topics}
            return {"status": "success", "plan": plan}
        else:
            plan = {"empresas": list(mentioned_companies), "topicos": AVAILABLE_TOPICS}
            return {"status": "success", "plan": plan}
    except Exception as e:
        plan = {"empresas": list(mentioned_companies), "topicos": AVAILABLE_TOPICS}
        return {"status": "success", "plan": plan}
        
def get_final_unified_answer(query, context):
    """Gera a resposta final usando o contexto recuperado."""
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-2.0-flash:generateContent?key={GEMINI_API_KEY}"
    
    has_complete_8_4 = "=== SE√á√ÉO COMPLETA DO ITEM 8.4" in context
    has_tagged_chunks = "=== CHUNKS COM TAGS ESPEC√çFICAS" in context
    
    if has_complete_8_4:
        structure_instruction = """
**ESTRUTURA OBRIGAT√ìRIA PARA ITEM 8.4:**
Use a estrutura oficial do item 8.4 do Formul√°rio de Refer√™ncia:
a) Termos e condi√ß√µes gerais dos planos
b) Data de aprova√ß√£o e √≥rg√£o respons√°vel
c) N√∫mero m√°ximo de a√ß√µes abrangidas pelos planos
d) N√∫mero m√°ximo de op√ß√µes a serem outorgadas
e) Condi√ß√µes de aquisi√ß√£o de a√ß√µes
f) Crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio
g) Crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio
h) Forma de liquida√ß√£o
i) Restri√ß√µes √† transfer√™ncia das a√ß√µes
j) Crit√©rios e eventos que, quando verificados, ocasionar√£o a suspens√£o, altera√ß√£o ou extin√ß√£o do plano
k) Efeitos da sa√≠da do administrador do cargo na manuten√ß√£o dos seus direitos no plano

Para cada subitem, extraia e organize as informa√ß√µes encontradas na SE√á√ÉO COMPLETA DO ITEM 8.4.
"""
    elif has_tagged_chunks:
        structure_instruction = "**PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS e organize a resposta de forma l√≥gica usando Markdown."
    else:
        structure_instruction = "Organize a resposta de forma l√≥gica e clara usando Markdown."
    
    prompt = f"""
Voc√™ √© um analista financeiro s√™nior especializado em Formul√°rios de Refer√™ncia da CVM. 

**PERGUNTA ORIGINAL DO USU√ÅRIO:**
"{query}"

**CONTEXTO COLETADO DOS DOCUMENTOS:**
{context}

{structure_instruction}

**INSTRU√á√ïES PARA O RELAT√ìRIO FINAL:**
1. Responda diretamente √† pergunta do usu√°rio
2. **PRIORIZE** as informa√ß√µes da SE√á√ÉO COMPLETA DO ITEM 8.4 quando dispon√≠vel
3. **PRIORIZE** as informa√ß√µes dos CHUNKS COM TAGS ESPEC√çFICAS quando dispon√≠vel
4. Use informa√ß√µes complementares apenas para esclarecer ou expandir pontos espec√≠ficos
5. Seja detalhado, preciso e profissional
6. Transcreva dados importantes como valores, datas e percentuais
7. Se alguma informa√ß√£o n√£o estiver dispon√≠vel, indique: "Informa√ß√£o n√£o encontrada nas fontes analisadas"
8. Mantenha a estrutura t√©cnica apropriada para administradores de LTIP

**RELAT√ìRIO ANAL√çTICO FINAL:**
"""
    
    payload = {
        "contents": [{"parts": [{"text": prompt}]}],
        "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}
    }
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        return f"ERRO ao gerar resposta final: {e}"

# --- INTERFACE STREAMLIT ---

def main():
    st.set_page_config(
        page_title="Agente de An√°lise LTIP",
        page_icon="üîç",
        layout="wide",
        initial_sidebar_state="expanded"
    )
    
    st.title("üîç Agente de An√°lise de Planos de Incentivo Longo Prazo ILP")
    st.markdown("---")
    
    # Carrega os artefatos
    with st.spinner("Inicializando sistema..."):
        loaded_artifacts, embedding_model, company_catalog = load_all_artifacts()
    
    if not loaded_artifacts:
        st.error("‚ùå Erro no carregamento dos artefatos. Verifique os arquivos na pasta 'dados'.")
        st.info("Certifique-se de que os arquivos FAISS (.bin) e chunks (.json) est√£o na pasta 'dados'.")
        return
    
    # Sidebar com informa√ß√µes
    with st.sidebar:
        st.header("üìä Informa√ß√µes do Sistema")
        st.metric("Fontes dispon√≠veis", len(loaded_artifacts))
        st.metric("Empresas identificadas", len(company_catalog))
        
        with st.expander("üìã Ver empresas dispon√≠veis"):
            for company in sorted(company_catalog):
                st.write(f"‚Ä¢ {company}")
        
        with st.expander("üìÅ Ver fontes de dados"):
            for source in loaded_artifacts.keys():
                st.write(f"‚Ä¢ {source}")
        
        st.markdown("---")
        st.markdown("### üîß Status do Sistema")
        st.success("‚úÖ Sistema carregado")
        st.info(f"ü§ñ Modelo: {MODEL_NAME}")
    
    # Interface principal
    st.header("üí¨ Fa√ßa sua pergunta")
    
    # Exemplos de perguntas
    with st.expander("üí° Exemplos de perguntas"):
        col1, col2 = st.columns(2)
        
        with col1:
            st.markdown("""
            **üéØ An√°lises espec√≠ficas:**
            - "Fale sobre o vesting e dividendos da CCR"
            - "Como funciona a liquida√ß√£o na Vibra?"
            - "Quais s√£o os crit√©rios de exerc√≠cio da Vale?"
            """)
        
        with col2:
            st.markdown("""
            **üìã An√°lises completas:**
            - "Mostre o item 8.4 completo da Vibra"
            - "Compare os planos entre CCR e Vibra"
            - "Resumo dos planos de stock options"
            """)
    
    # Input da pergunta
    user_query = st.text_area(
        "Digite sua pergunta sobre planos de incentivo:",
        height=100,
        placeholder="Ex: Fale sobre o vesting e dividendos da CCR",
        help="Seja espec√≠fico sobre a empresa e o t√≥pico de interesse"
    )
    
    # Bot√µes de a√ß√£o
    col1, col2, col3 = st.columns([1, 2, 1])
    
    with col2:
        analyze_button = st.button("üîç Analisar", type="primary", use_container_width=True)
    
    if analyze_button:
        if not user_query.strip():
            st.warning("‚ö†Ô∏è Por favor, digite uma pergunta.")
            return
        
        # Processo de an√°lise
        with st.container():
            st.markdown("---")
            st.subheader("üìã Processo de An√°lise")
            
            # Etapa 1: Planejamento
            with st.status("1Ô∏è‚É£ Gerando plano de an√°lise...", expanded=True) as status:
                plan_response = create_dynamic_analysis_plan(
                    user_query, company_catalog, list(loaded_artifacts.keys())
                )
                
                if plan_response['status'] != 'success':
                    st.error("‚ùå Erro ao gerar plano de an√°lise")
                    return
                
                plan = plan_response['plan']
                
                if plan.get('empresas'):
                    st.write(f"**üè¢ Empresas identificadas:** {', '.join(plan.get('empresas', []))}")
                else:
                    st.write("**üè¢ Empresas identificadas:** Nenhuma")
                
                st.write(f"**üìù T√≥picos a analisar:** {len(plan.get('topicos', []))}")
                
                if plan.get('topicos'):
                    with st.expander("Ver t√≥picos identificados"):
                        for i, topico in enumerate(plan.get('topicos', [])[:10], 1):
                            st.write(f"{i}. {topico}")
                
                status.update(label="‚úÖ Plano gerado com sucesso!", state="complete")
            
            if not plan.get("empresas"):
                st.error("‚ùå N√£o consegui identificar empresas na sua pergunta. Seja mais espec√≠fico.")
                st.info("üí° Dica: Mencione o nome da empresa claramente (ex: CCR, Vibra, Petrobras)")
                return
            
            # Etapa 2: Recupera√ß√£o de contexto
            with st.status("2Ô∏è‚É£ Recuperando contexto relevante...", expanded=True) as status:
                query_intent = 'item_8_4_query' if any(term in user_query.lower() for term in ['8.4', '8-4', 'item 8.4', 'formul√°rio']) else 'general_query'
                
                st.write(f"**üéØ Estrat√©gia detectada:** {'Item 8.4 completo' if query_intent == 'item_8_4_query' else 'Busca geral'}")
                
                retrieved_context, sources = execute_dynamic_plan(
                    plan, query_intent, loaded_artifacts, embedding_model
                )
                
                if not retrieved_context.strip():
                    st.error("‚ùå N√£o encontrei informa√ß√µes relevantes nos documentos.")
                    return
                
                st.write(f"**üìÑ Contexto recuperado de:** {len(set(sources))} documento(s)")
                status.update(label="‚úÖ Contexto recuperado com sucesso!", state="complete")
            
            # Etapa 3: Gera√ß√£o da resposta
            with st.status("3Ô∏è‚É£ Gerando resposta final...", expanded=True) as status:
                final_answer = get_final_unified_answer(user_query, retrieved_context)
                status.update(label="‚úÖ An√°lise conclu√≠da!", state="complete")
        
        # Resultado final
        st.markdown("---")
        st.subheader("üìÑ Resultado da An√°lise")
        
        # Exibe a resposta em um container
        with st.container():
            st.markdown(final_answer)
        
        # Fontes consultadas
        if sources:
            st.markdown("---")
            with st.expander(f"üìö Documentos consultados ({len(set(sources))})", expanded=False):
                unique_sources = sorted(set(sources))
                for i, source in enumerate(unique_sources, 1):
                    st.write(f"{i}. {source}")
        
        # Bot√£o para nova consulta
        st.markdown("---")
        if st.button("üîÑ Nova Consulta", use_container_width=True):
            st.rerun()

if __name__ == "__main__":
    main()
