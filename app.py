# app.py
# -*- coding: utf-8 -*-
"""
AGENTE COM PLANEJAMENTO DIN√ÇMICO - VERS√ÉO FINAL STREAMLIT (MAP-REDUCE)

Este script implementa o agente de an√°lise de LTIP como uma aplica√ß√£o web interativa
usando Streamlit. A arquitetura foi refatorada para usar uma estrat√©gia "Map-Reduce",
permitindo a an√°lise de grandes volumes de documentos de forma escal√°vel e evitando
erros de limite de requisi√ß√µes (429) da API.
"""

# --- IMPORTA√á√ïES ---
import streamlit as st
import json
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import requests
import glob
import os
import re
import unicodedata
import time

# --- CONFIGURA√á√ÉO DA P√ÅGINA STREAMLIT ---
st.set_page_config(page_title="Agente de An√°lise LTIP", layout="wide")
st.title("ü§ñ Agente de An√°lise de Planos de Incentivo")
st.caption("Uma aplica√ß√£o que usa IA para analisar Formul√°rios de Refer√™ncia da CVM de forma escal√°vel.")

# --- CONFIGURA√á√ïES E ESTRUTURAS DE CONHECIMENTO ---
MODEL_NAME = 'sentence-transformers/all-MiniLM-L6-v2'
TOP_K_SEARCH = 7
# O caminho aponta para a pasta 'dados' no reposit√≥rio do GitHub
GOOGLE_DRIVE_PATH = './dados'

# Dicion√°rio especializado para termos t√©cnicos de LTIP
TERMOS_TECNICOS_LTIP = {
    "tratamento de dividendos": ["tratamento de dividendos", "equivalente em dividendos", "dividendos", "juros sobre capital pr√≥prio", "proventos", "dividend equivalent", "dividendos pagos em a√ß√µes", "ajustes por dividendos"],
    "pre√ßo de exerc√≠cio": ["pre√ßo de exerc√≠cio", "strike price", "pre√ßo de compra", "pre√ßo fixo", "valor de exerc√≠cio", "pre√ßo pr√©-estabelecido", "pre√ßo de aquisi√ß√£o"],
    "forma de liquida√ß√£o": ["forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "entrega f√≠sica", "pagamento em dinheiro", "transfer√™ncia de a√ß√µes", "settlement"],
    "vesting": ["vesting", "per√≠odo de car√™ncia", "car√™ncia", "aquisi√ß√£o de direitos", "cronograma de vesting", "vesting schedule", "per√≠odo de cliff"],
    "eventos corporativos": ["eventos corporativos", "desdobramento", "grupamento", "dividendos pagos em a√ß√µes", "bonifica√ß√£o", "split", "ajustes", "reorganiza√ß√£o societ√°ria"],
    "stock options": ["stock options", "op√ß√µes de a√ß√µes", "op√ß√µes de compra", "SOP", "plano de op√ß√µes", "ESOP", "op√ß√£o de compra de a√ß√µes"],
    "a√ß√µes restritas": ["a√ß√µes restritas", "restricted shares", "RSU", "restricted stock units", "a√ß√µes com restri√ß√£o", "plano de a√ß√µes restritas"]
}

# T√≥picos expandidos de an√°lise que o planejador pode usar
AVAILABLE_TOPICS = [
    "termos e condi√ß√µes gerais", "data de aprova√ß√£o e √≥rg√£o respons√°vel", "n√∫mero m√°ximo de a√ß√µes abrangidas", "n√∫mero m√°ximo de op√ß√µes a serem outorgadas", "condi√ß√µes de aquisi√ß√£o de a√ß√µes", "crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o ou exerc√≠cio", "pre√ßo de exerc√≠cio", "strike price", "crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o ou exerc√≠cio", "forma de liquida√ß√£o", "liquida√ß√£o", "pagamento", "restri√ß√µes √† transfer√™ncia das a√ß√µes", "crit√©rios e eventos de suspens√£o/extin√ß√£o", "efeitos da sa√≠da do administrador", "Tipos de Planos", "Condi√ß√µes de Car√™ncia", "Vesting", "per√≠odo de car√™ncia", "cronograma de vesting", "Matching", "contrapartida", "co-investimento", "Lockup", "per√≠odo de lockup", "restri√ß√£o de venda", "Tratamento de Dividendos", "equivalente em dividendos", "proventos", "Stock Options", "op√ß√µes de a√ß√µes", "SOP", "A√ß√µes Restritas", "RSU", "restricted shares", "Eventos Corporativos", "IPO", "grupamento", "desdobramento"
]

# --- FUN√á√ïES DE L√ìGICA ---

def expand_search_terms(base_term):
    expanded_terms = [base_term.lower()]
    for category, terms in TERMOS_TECNICOS_LTIP.items():
        if any(term.lower() in base_term.lower() for term in terms):
            expanded_terms.extend([term.lower() for term in terms])
    return list(set(expanded_terms))

def search_by_tags(artifacts, company_name, target_tags):
    results = []
    for index_name, artifact_data in artifacts.items():
        chunk_data = artifact_data['chunks']
        for i, mapping in enumerate(chunk_data.get('map', [])):
            document_path = mapping['document_path']
            if re.search(re.escape(company_name.split(' ')[0]), document_path, re.IGNORECASE):
                chunk_text = chunk_data["chunks"][i]
                for tag in target_tags:
                    if f"T√≥picos:" in chunk_text and tag in chunk_text:
                        results.append({'text': chunk_text, 'path': document_path, 'source': index_name, 'tag_found': tag})
                        break
    return results

# --- FUN√á√ïES DE CARREGAMENTO E CACHE ---
@st.cache_resource
def load_all_artifacts():
    artifacts = {}
    canonical_company_names = set()
    with st.spinner("Carregando modelo de embedding (isso s√≥ acontece na primeira vez)..."):
        model = SentenceTransformer(MODEL_NAME)
    
    index_files = glob.glob(os.path.join(GOOGLE_DRIVE_PATH, '*_faiss_index.bin'))
    if not index_files:
        st.error(f"ERRO CR√çTICO: Nenhum arquivo de √≠ndice (*_faiss_index.bin) encontrado na pasta '{GOOGLE_DRIVE_PATH}'. Verifique se os arquivos de dados est√£o na pasta 'dados' no GitHub e se o nome da pasta est√° em min√∫sculas.")
        return None, None, None

    for index_file in index_files:
        category = os.path.basename(index_file).replace('_faiss_index.bin', '')
        chunks_file = os.path.join(GOOGLE_DRIVE_PATH, f"{category}_chunks_map.json")
        try:
            index = faiss.read_index(index_file)
            with open(chunks_file, 'r', encoding='utf-8') as f:
                chunk_data = json.load(f)
            artifacts[category] = {'index': index, 'chunks': chunk_data}
            for mapping in chunk_data.get('map', []):
                
                parts = mapping['document_path'].split('/')
                if parts:
                    canonical_company_names.add(parts[0])
        except FileNotFoundError:
            st.warning(f"AVISO: Arquivo de chunks '{chunks_file}' n√£o encontrado.")
            continue
    
    if not artifacts:
        st.error("ERRO CR√çTICO: Nenhum artefato foi carregado com sucesso.")
        return None, None, None

    return artifacts, model, list(canonical_company_names)

# --- FUN√á√ïES DE INTERA√á√ÉO COM A IA (PLANEJADOR E EXECUTOR) ---

@st.cache_data
def create_dynamic_analysis_plan(_query, company_catalog, available_indices):
    api_key = st.secrets["GEMINI_API_KEY"]
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}"
    
    def normalize_name(name):
        try:
            nfkd_form = unicodedata.normalize('NFKD', name.lower())
            name = "".join([c for c in nfkd_form if not unicodedata.combining(c)])
            name = re.sub(r'[.,-]', '', name)
            suffixes = [r'\bs\.?a\.?\b', r'\bltda\b', r'\bholding\b', r'\bparticipacoes\b', r'\bcia\b', r'\bind\b', r'\bcom\b']
            for suffix in suffixes:
                name = re.sub(suffix, '', name, flags=re.IGNORECASE)
            return re.sub(r'\s+', '', name).strip()
        except Exception: return name.lower()

    mentioned_companies = []
    query_clean = _query.lower().strip()
    
    for canonical_name in company_catalog:
        if (canonical_name.lower() in query_clean or any(len(part) > 2 and re.search(r'\b' + re.escape(part.lower()) + r'\b', query_clean) for part in canonical_name.split(' ')) or (len(query_clean) > 2 and normalize_name(query_clean) in normalize_name(canonical_name))):
            if canonical_name not in mentioned_companies: mentioned_companies.append(canonical_name)

    if not mentioned_companies and len(query_clean) <= 6:
        for canonical_name in company_catalog:
            if query_clean.upper() in canonical_name.upper():
                if canonical_name not in mentioned_companies: mentioned_companies.append(canonical_name)
    
    prompt = f'Voc√™ √© um planejador de an√°lise. Sua tarefa √© analisar a "Pergunta do Usu√°rio" e identificar os t√≥picos de interesse. Se a pergunta for gen√©rica (ex: "resumo dos planos"), inclua todos os "T√≥picos de An√°lise Dispon√≠veis". Se for espec√≠fica (ex: "fale sobre o vesting"), inclua apenas os t√≥picos relevantes. Retorne APENAS uma lista JSON de strings contendo os t√≥picos. T√≥picos de An√°lise Dispon√≠veis: {json.dumps(AVAILABLE_TOPICS)}. Pergunta do Usu√°rio: "{_query}". T√≥picos de Interesse (responda APENAS com a lista JSON):'
    
    payload = {"contents": [{"parts": [{"text": prompt}]}]}
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        text_response = response.json()['candidates'][0]['content']['parts'][0]['text']
        json_match = re.search(r'\[.*\]', text_response, re.DOTALL)
        if json_match:
            topics = json.loads(json_match.group(0))
            plan = {"empresas": list(set(mentioned_companies)), "topicos": topics}
            return {"status": "success", "plan": plan}
    except Exception as e:
        st.warning(f"Erro no planejamento, usando fallback. Detalhe: {e}")
    
    plan = {"empresas": list(set(mentioned_companies)), "topicos": AVAILABLE_TOPICS}
    return {"status": "success", "plan": plan}


def execute_dynamic_plan(plan, query_intent, artifacts, model):
    full_context = ""
    all_retrieved_docs = set()
    for empresa in plan.get("empresas", []):
        full_context += f"--- IN√çCIO DA AN√ÅLISE PARA: {empresa.upper()} ---\n\n"
        if query_intent == 'item_8_4_query' and 'item_8_4' in artifacts:
            artifact_data = artifacts['item_8_4']
            chunk_data = artifact_data['chunks']
            for i, mapping in enumerate(chunk_data.get('map', [])):
                if re.search(re.escape(empresa.split(' ')[0]), mapping['document_path'], re.IGNORECASE):
                    all_retrieved_docs.add(mapping['document_path'])
                    full_context += f"--- Chunk Item 8.4 (Doc: {mapping['document_path']}) ---\n{chunk_data['chunks'][i]}\n\n"
        
        target_tags = []
        for topico in plan.get("topicos", []): target_tags.extend(expand_search_terms(topico))
        target_tags = list(set([tag.title() for tag in target_tags if len(tag) > 3]))
        tagged_chunks = search_by_tags(artifacts, empresa, target_tags)
        for chunk_info in tagged_chunks:
            full_context += f"--- Chunk com tag '{chunk_info['tag_found']}' (Doc: {chunk_info['path']}) ---\n{chunk_info['text']}\n\n"
            all_retrieved_docs.add(chunk_info['path'])
    return full_context, [str(doc) for doc in all_retrieved_docs]

# --- FUN√á√ïES DE S√çNTESE (ESTRAT√âGIA MAP-REDUCE) ---

def summarize_chunk(chunk_text, query):
    """(MAP) Pede √† IA para resumir um √∫nico chunk de texto."""
    api_key = st.secrets["GEMINI_API_KEY"]
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}"
    prompt = f'Com base na pergunta do usu√°rio e no chunk de texto abaixo, extraia e resuma APENAS as informa√ß√µes relevantes. Se o chunk n√£o contiver informa√ß√µes relevantes, responda com "N/A".\n\nPergunta do Usu√°rio: "{query}"\n\nChunk de Texto:\n---\n{chunk_text}\n---\n\nResumo Conciso das Informa√ß√µes Relevantes:'
    payload = {"contents": [{"parts": [{"text": prompt}]}], "generationConfig": {"temperature": 0.0, "maxOutputTokens": 1024}}
    headers = {'Content-Type': 'application/json'}
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=90)
        response.raise_for_status()
        summary = response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
        if summary.upper() != "N/A" and len(summary) > 20:
            return summary
    except Exception: return None
    return None

def get_final_unified_answer(query, context, plan):
    """(REDUCE) Sintetiza a resposta final usando a estrat√©gia Map-Reduce."""
    chunks = re.split(r'--- Chunk|--- IN√çCIO DA AN√ÅLISE PARA:', context)
    relevant_chunks = [chunk.strip() for chunk in chunks if len(chunk.strip()) > 100]

    st.info(f"Analisando {len(relevant_chunks)} chunks de informa√ß√£o relevantes...")
    summaries = []
    progress_bar = st.progress(0, text="Mapeando e resumindo chunks...")

    for i, chunk in enumerate(relevant_chunks):
        summary = summarize_chunk(chunk, query)
        if summary: summaries.append(summary)
        # Controle de Rate Limit: 1.1s de espera para ficar abaixo do limite de 60 RPM
        time.sleep(1.1)
        progress_bar.progress((i + 1) / len(relevant_chunks), text=f"Mapeando e resumindo chunks... ({i+1}/{len(relevant_chunks)})")
    
    progress_bar.empty()

    if not summaries:
        return "N√£o foi poss√≠vel extrair informa√ß√µes relevantes dos documentos para montar um relat√≥rio."

    st.info("Todos os chunks foram analisados. Sintetizando o relat√≥rio final...")
    final_context = "\n\n---\n\n".join(summaries)
    api_key = st.secrets["GEMINI_API_KEY"]
    url = f"https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash-latest:generateContent?key={api_key}"
    final_prompt = f'Voc√™ √© um analista financeiro s√™nior. Sua tarefa √© criar um relat√≥rio coeso e bem estruturado respondendo √† pergunta do usu√°rio. Use os resumos de contexto fornecidos abaixo, que foram extra√≠dos de v√°rios documentos. Sintetize as informa√ß√µes em uma √∫nica resposta final. N√£o liste os resumos, use-os para construir seu texto.\n\nPergunta Original do Usu√°rio: "{query}"\n\nResumos de Contexto para usar como base:\n---\n{final_context}\n---\n\nRelat√≥rio Anal√≠tico Final (responda de forma completa e profissional):'
    payload = {"contents": [{"parts": [{"text": final_prompt}]}], "generationConfig": {"temperature": 0.1, "maxOutputTokens": 8192}}
    headers = {'Content-Type': 'application/json'}
    
    try:
        response = requests.post(url, headers=headers, data=json.dumps(payload), timeout=180)
        response.raise_for_status()
        return response.json()['candidates'][0]['content']['parts'][0]['text'].strip()
    except Exception as e:
        return f"ERRO ao gerar a s√≠ntese final do relat√≥rio: {e}"

# --- L√ìGICA PRINCIPAL DA APLICA√á√ÉO STREAMLIT ---
try:
    loaded_artifacts, embedding_model, company_catalog = load_all_artifacts()
except Exception as e:
    st.error(f"Ocorreu um erro fatal durante o carregamento inicial dos recursos: {e}")
    st.stop()

if loaded_artifacts:
    if "messages" not in st.session_state:
        st.session_state.messages = [{"role": "assistant", "content": "Ol√°! Qual empresa ou plano de incentivo voc√™ gostaria de analisar?"}]

    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Digite sua pergunta sobre CCR, Vibra, etc."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            with st.spinner("Iniciando an√°lise..."):
                plan_response = create_dynamic_analysis_plan(prompt, company_catalog, list(loaded_artifacts.keys()))
                
                if plan_response['status'] != 'success' or not plan_response['plan'].get("empresas"):
                    response_text = "N√£o consegui identificar uma empresa em sua pergunta ou houve um erro no planejamento. Por favor, seja mais espec√≠fico."
                else:
                    plan = plan_response['plan']
                    st.info(f"Plano de an√°lise criado. Foco em: {plan['empresas'][0] if plan['empresas'] else 'N/A'}.")
                    
                    query_intent = 'item_8_4_query' if any(term in prompt.lower() for term in ['8.4', '8-4', 'item 8.4', 'formul√°rio']) else 'general_query'
                    retrieved_context, sources = execute_dynamic_plan(plan, query_intent, loaded_artifacts, embedding_model)

                    if not retrieved_context.strip():
                        response_text = "N√£o encontrei informa√ß√µes relevantes nos documentos para a sua solicita√ß√£o."
                    else:
                        final_answer = get_final_unified_answer(prompt, retrieved_context, plan)
                        response_text = final_answer
                        if sources:
                            unique_sources = sorted(list(set(sources)))
                            with st.expander(f"Fontes Consultadas ({len(unique_sources)})"):
                                for source in unique_sources:
                                    st.write(f"- {source}")
            st.markdown(response_text)
        st.session_state.messages.append({"role": "assistant", "content": response_text})

