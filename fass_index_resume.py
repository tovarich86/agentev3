# 1. For√ßa a atualiza√ß√£o do torch e transformers para as vers√µes mais recentes e seguras
!pip install --upgrade torch torchvision torchaudio "transformers>=4.42.0" "sentence-transformers>=3.0.0"

# 2. Instala a vers√£o da GPU do FAISS a partir do reposit√≥rio confi√°vel da NVIDIA
!pip install faiss-gpu-cu12 --pre -f https://pypi.nvidia.com

DICIONARIO_UNIFICADO_HIERARQUICO = {
    "FormularioReferencia_Item_8_4": {
        "a_TermosGerais": {"aliases": ["termos e condi√ß√µes gerais", "objetivos do plano", "eleg√≠veis", "principais regras"], "subtopicos": {}},
        "b_Aprovacao": {"aliases": ["data de aprova√ß√£o", "√≥rg√£o respons√°vel", "assembleia geral"], "subtopicos": {}},
        "c_MaximoAcoes": {"aliases": ["n√∫mero m√°ximo de a√ß√µes abrangidas", "dilui√ß√£o m√°xima"], "subtopicos": {}},
        "d_MaximoOpcoes": {"aliases": ["n√∫mero m√°ximo de op√ß√µes a serem outorgadas", "limite de op√ß√µes"], "subtopicos": {}},
        "e_CondicoesAquisicao": {"aliases": ["condi√ß√µes de aquisi√ß√£o de a√ß√µes", "metas de desempenho", "tempo de servi√ßo"], "subtopicos": {}},
        "f_CriteriosPreco": {"aliases": ["crit√©rios para fixa√ß√£o do pre√ßo de aquisi√ß√£o", "pre√ßo de exerc√≠cio", "pre√ßo fixo previamente estabelecido"], "subtopicos": {}},
        "g_CriteriosPrazo": {"aliases": ["crit√©rios para fixa√ß√£o do prazo de aquisi√ß√£o", "prazo de exerc√≠cio"], "subtopicos": {}},
        "h_FormaLiquidacao": {"aliases": ["forma de liquida√ß√£o", "pagamento em dinheiro", "entrega f√≠sica das a√ß√µes", "entrega de a√ß√µes"], "subtopicos": {}},
        "i_RestricoesTransferencia": {"aliases": ["restri√ß√µes √† transfer√™ncia", "per√≠odos de bloqueio", "lockup", "bloqueio", "per√≠odo de restri√ß√£o √† negocia√ß√£o"], "subtopicos": {}},
        "j_SuspensaoExtincao": {"aliases": ["suspens√£o, altera√ß√£o ou extin√ß√£o do plano", "mudan√ßas nas pol√≠ticas"], "subtopicos": {}},
        "k_EfeitosSaida": {"aliases": ["efeitos da sa√≠da do administrador", "regras de desligamento", "aposentadoria", "demiss√£o"], "subtopicos": {}},
    },
    "TiposDePlano": {
        "AcoesRestritas": {
            "aliases": ["A√ß√µes Restritas", "Restricted Shares", "RSU"],
            "subtopicos": {
                "PerformanceShares": {
                    "aliases": ["Performance Shares", "PSU", "A√ß√µes de Performance"],
                    "subtopicos": {}
                }
            }
        },
        "OpcoesDeCompra": {
            "aliases": ["Op√ß√µes de Compra", "Stock Options", "ESOP", "SOP"],
            "subtopicos": {}
        },
        "PlanoCompraAcoes_ESPP": {
            "aliases": ["Plano de Compra de A√ß√µes", "Employee Stock Purchase Plan", "ESPP"],
            "subtopicos": {
                "Matching_Coinvestimento": {
                    "aliases": ["Matching", "Contrapartida", "Co-investimento", "Plano de Matching"],
                    "subtopicos": {}
                }
            }
        },
        "AcoesFantasmas": {
            "aliases": ["A√ß√µes Fantasmas", "Phantom Shares", "A√ß√µes Virtuais"],
            "subtopicos": {}
        },
        "OpcoesFantasmas_SAR": {
            "aliases": ["Op√ß√µes Fantasmas", "Phantom Options", "SAR", "Share Appreciation Rights", "Direito √† Valoriza√ß√£o de A√ß√µes"],
            "subtopicos": {}
        },
        "BonusRetencaoDiferido": {
            "aliases": ["B√¥nus de Reten√ß√£o", "B√¥nus de Perman√™ncia", "Staying Bonus", "Retention Bonus", "Deferred Bonus"],
            "subtopicos": {}
        }
    },
    "MecanicasCicloDeVida": {
        "Outorga": {"aliases": ["Outorga", "Concess√£o", "Grant", "Grant Date"], "subtopicos": {}},
        "Vesting": {"aliases": ["Vesting", "Per√≠odo de Car√™ncia", "Aquisi√ß√£o de Direitos", "cronograma de vesting", "Vesting Gradual"], "subtopicos": {}},
        "VestingCliff": {"aliases": ["Cliff", "Cliff Period", "Per√≠odo de Cliff", "Car√™ncia Inicial"], "subtopicos": {}},
        "VestingAcelerado": {"aliases": ["Vesting Acelerado", "Accelerated Vesting", "Cl√°usula de Acelera√ß√£o", "antecipa√ß√£o do vesting"], "subtopicos": {}},
        "VestingTranche": {"aliases": ["Tranche", "Lote", "Parcela do Vesting", 'Parcela', "Anivers√°rio"], "subtopicos": {}},
        "PrecoExercicio": {"aliases": ["Pre√ßo de Exerc√≠cio", "Strike", "Strike Price"], "subtopicos": {}},
        "PrecoDesconto": {"aliases": ["Desconto de", "pre√ßo com desconto", "desconto sobre o pre√ßo"], "subtopicos": {}},
        "CicloExercicio": {"aliases": ["Exerc√≠cio", "Per√≠odo de Exerc√≠cio", "pagamento", "liquida√ß√£o", "vencimento", "expira√ß√£o"], "subtopicos": {}},
        "Lockup": {"aliases": ["Lockup", "Per√≠odo de Lockup", "Restri√ß√£o de Venda"], "subtopicos": {}},
    },
    "GovernancaRisco": {
        "DocumentosPlano": {"aliases": ["Regulamento", "Regulamento do Plano", "Contrato de Ades√£o", "Termo de Outorga"], "subtopicos": {}},
        "OrgaoDeliberativo": {"aliases": ["Comit√™ de Remunera√ß√£o", "Comit√™ de Pessoas", "Delibera√ß√£o do Conselho", "Conselho de Administra√ß√£o"], "subtopicos": {}},
        "MalusClawback": {"aliases": ["Malus", "Clawback", "Cl√°usula de Recupera√ß√£o", "Forfeiture", "SOG", "Stock Ownership Guidelines"], "subtopicos": {}},
        "Diluicao": {"aliases": ["Dilui√ß√£o", "Dilution", "Capital Social", "Fator de Dilui√ß√£o"], "subtopicos": {}},
        "NaoConcorrencia": {
            "aliases": ["Non-Compete", "N√£o-Competi√ß√£o", "Garden-leave", "obriga√ß√£o de n√£o concorrer", "proibi√ß√£o de competi√ß√£o"],
            "subtopicos": {}
        }
    },
    "ParticipantesCondicoes": {
        "Elegibilidade": {"aliases": ["Participantes", "Benefici√°rios", "Eleg√≠veis", "Empregados", "Administradores", "Colaboradores", "Executivos", "Diretores", "Gerentes", "Conselheiros"], "subtopicos": {}},
        "CondicaoSaida": {"aliases": ["Desligamento", "Sa√≠da", "T√©rmino do Contrato", "Rescis√£o", "Demiss√£o", "Good Leaver", "Bad Leaver"], "subtopicos": {}},
        "CasosEspeciais": {"aliases": ["Aposentadoria", "Morte", "Invalidez", "Afastamento"], "subtopicos": {}},
    },
    "IndicadoresPerformance": {
        "ConceitoGeral_Performance": {
            "aliases": ["Plano de Desempenho", "Metas de Performance", "crit√©rios de desempenho", "metas", "indicadores de performance", "metas", "performance"],
            "subtopicos": {
                "Financeiro": {
                    "aliases": [
                        "ROIC", "EBITDA", "LAIR", "Lucro", "CAGR", "Receita L√≠quida",
                        "fluxo de caixa", "gera√ß√£o de caixa", "Free Cash Flow", "FCF",
                        "lucros por a√ß√£o", "Earnings per Share", "EPS", "redu√ß√£o de d√≠vida",
                        "D√≠vida L√≠quida / EBITDA", "capital de giro", "retorno sobre investimentos",
                        "retorno sobre capital", "Return on Investment", "ROCE",
                        "margem bruta", "margem operacional", "lucro l√≠quido",
                        "lucro operacional", "receita operacional", "vendas l√≠quidas",
                        "valor econ√¥mico agregado", "custo de capital", "WACC",
                        "Weighted Average Capital Cost", "retorno sobre ativo",
                        "retorno sobre ativo l√≠quido", "rotatividade de ativos l√≠quidos",
                        "rotatividade do estoque", "despesas de capital", "d√≠vida financeira bruta",
                        "receita operacional l√≠quida", "lucros por a√ß√£o dilu√≠dos",
                        "lucros por a√ß√£o b√°sicos", "rentabilidade", "Enterprise Value", "EV",
                        "Valor Te√≥rico da Companhia", "Valor Te√≥rico Unit√°rio da A√ß√£o",
                        "Economic Value Added", "EVA", "NOPAT", "Net Operating Profit After Tax",
                        "Capital Total Investido", "CAGR EBITDA per Share", "Equity Value"
                    ],
                    "subtopicos": {}
                },
                "Mercado": {
                    "aliases": ["CDI", "IPCA", "Selic"],
                    "subtopicos": {}
                },
                "TSR": {
                    "aliases": ["TSR", "Total Shareholder Return", "Retorno Total ao Acionista"],
                    "subtopicos": {
                        "TSR_Absoluto": {"aliases": ["TSR Absoluto"], "subtopicos": {}},
                        "TSR_Relativo": {"aliases": ["TSR Relativo", "Relative TSR", "TSR versus", "TSR comparado a"], "subtopicos": {}}
                    }
                },
                "ESG": {
                    "aliases": ["Metas ESG", "ESG", "Neutraliza√ß√£o de Emiss√µes", "Redu√ß√£o de Emiss√µes", "Igualdade de G√™nero", "objetivos de desenvolvimento sustent√°vel", "IAGEE", "ICMA"],
                    "subtopicos": {}
                },
                "Operacional": {
                    "aliases": ["produtividade", "efici√™ncia operacional", "desempenho de entrega", "desempenho de seguran√ßa", "qualidade", "satisfa√ß√£o do cliente", "NPS", "conclus√£o de aquisi√ß√µes", "expans√£o comercial", "crescimento"],
                    "subtopicos": {}
                }
            }
        },
        "GrupoDeComparacao": {"aliases": ["Peer Group", "Empresas Compar√°veis", "Companhias Compar√°veis"], "subtopicos": {}}
    },
    "EventosFinanceiros": {
        "EventosCorporativos": {"aliases": ["grupamento", "desdobramento", "cis√£o", "fus√£o", "incorpora√ß√£o", "bonifica√ß√£o"], "subtopicos": {}},
        "MudancaDeControle": {"aliases": ["Mudan√ßa de Controle", "Change of Control", "Transfer√™ncia de Controle"], "subtopicos": {}},
        "DividendosProventos": {"aliases": ["Dividendos", "JCP", "Juros sobre capital pr√≥prio", "dividend equivalent", "proventos"], "subtopicos": {}},
        "EventosDeLiquidez": {
            "aliases": ["Evento de Liquidez", "liquida√ß√£o antecipada", "sa√≠da da companhia", "transa√ß√£o de controle", "reorganiza√ß√£o societ√°ria", "desinvestimento", "deslistagem", "Opera√ß√£o Relevante"],
            "subtopicos": {
                "IPO_OPI": {"aliases": ["IPO", "Oferta P√∫blica Inicial", "Oferta Publica Inicial", "abertura do capital"], "subtopicos": {}},
                "AlienacaoControle": {"aliases": ["Aliena√ß√£o de Controle", "aliena√ß√£o de mais de 50% de a√ß√µes ordin√°rias", "venda de controle", "transfer√™ncia de controle acion√°rio", "venda ou permuta de A√ß√µes"], "subtopicos": {}},
                "FusaoAquisicaoVenda": {"aliases": ["Fus√£o", "Aquisi√ß√£o", "Incorpora√ß√£o", "Venda da Companhia", "venda, loca√ß√£o, arrendamento, cess√£o, licenciamento, transfer√™ncia ou qualquer outra forma de disposi√ß√£o da totalidade ou de parte substancial dos ativos"], "subtopicos": {}},
                "InvestimentoRelevante": {"aliases": ["investimento prim√°rio de terceiros", "aumento de capital", "Capitaliza√ß√£o da Companhia"], "subtopicos": {}}
            }
        }
    },
    "AspectosFiscaisContabeis": {
        "TributacaoEncargos": {"aliases": ["Encargos", "Impostos", "Tributa√ß√£o", "Natureza Mercantil", "Natureza Remunerat√≥ria", "INSS", "IRRF"], "subtopicos": {}},
        "NormasContabeis": {"aliases": ["IFRS 2", "CPC 10", "Valor Justo", "Fair Value", "Black-Scholes", "Despesa Cont√°bil", "Volatilidade"], "subtopicos": {}},
    }
}

# enrich_and_overwrite.py
#
# PASSO 1 (Revisado) do pipeline de processamento.
# Este script l√™ o arquivo de dados principal, enriquece-o com metadados
# de 'setor' e 'controle_acionario' e SOBRESCREVE o arquivo original
# com a vers√£o enriquecida, mantendo o nome do arquivo para os pr√≥ximos passos.

import json
import re
import logging
import pandas as pd
from pathlib import Path

# --- Configura√ß√µes ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

PROJECT_DRIVE_PATH = Path('/content/drive/MyDrive/Projeto_CVM_RAG')

# ATEN√á√ÉO: O arquivo de entrada e sa√≠da s√£o os mesmos para sobrescrita.
MAIN_DATA_FILE = PROJECT_DRIVE_PATH / "cvm_documentos_texto_final.json"
CATALOGO_EMPRESAS_CSV = PROJECT_DRIVE_PATH / "Identifica√ß√£o empresas.csv"

# --- FUN√á√ïES ---

def normalize_company_name(name: str) -> str:
    """Normaliza o nome da empresa para uma chave de consulta consistente."""
    if not isinstance(name, str): return ""
    name = re.sub(r'\s+S\.?/?A\.?$', '', name.upper().strip(), flags=re.IGNORECASE)
    return name

def create_enrichment_catalog(csv_path: str) -> dict:
    """Cria um cat√°logo a partir do CSV para o enriquecimento dos dados."""
    try:
        df = pd.read_csv(csv_path, sep=';')
        catalog = {}
        for _, row in df.iterrows():
            nome_atual = row['Nome_Empresarial'].upper().strip()
            company_data = {
                "harmonized_name": nome_atual,
                "setor": row['Setor_Atividade'],
                "controle_acionario": row['Especie_Controle_Acionario']
            }
            key_atual = normalize_company_name(nome_atual)
            if key_atual:
                catalog[key_atual] = company_data
            nome_anterior = row['Nome_Empresarial_Anterior']
            if isinstance(nome_anterior, str) and nome_anterior.strip():
                key_anterior = normalize_company_name(nome_anterior)
                if key_anterior:
                    catalog[key_anterior] = company_data
        logging.info(f"Cat√°logo de enriquecimento criado com {len(catalog)} chaves.")
        return catalog
    except Exception as e:
        logging.error(f"Erro ao criar o cat√°logo de enriquecimento: {e}")
        return {}

# --- FUN√á√ÉO PRINCIPAL ---

def enrich_and_overwrite_documents(data_file_path, catalog):
    """
    L√™ o JSON de documentos, enriquece os dados com metadados e
    sobrescreve o arquivo original.
    """
    try:
        logging.info(f"Lendo o arquivo de documentos de entrada: {data_file_path}")
        with open(data_file_path, 'r', encoding='utf-8') as f:
            documents_data = json.load(f)

        enriched_data = {}
        enriched_count = 0
        not_found_count = 0

        for url, doc_data in documents_data.items():
            original_name = doc_data.get("company_name", "")
            if not original_name:
                enriched_data[url] = doc_data
                continue

            normalized_name = normalize_company_name(original_name)
            company_info = catalog.get(normalized_name)

            if company_info:
                doc_data["company_name"] = company_info["harmonized_name"]
                doc_data["setor"] = company_info["setor"]
                doc_data["controle_acionario"] = company_info["controle_acionario"]
                enriched_count += 1
            else:
                doc_data["company_name"] = original_name.upper().strip()
                doc_data["setor"] = "N√£o Identificado"
                doc_data["controle_acionario"] = "N√£o Identificado"
                not_found_count += 1

            enriched_data[url] = doc_data

        logging.info(f"Processamento conclu√≠do. Documentos enriquecidos: {enriched_count}. N√£o encontrados: {not_found_count}.")

        logging.info(f"Sobrescrevendo o arquivo com dados enriquecidos: {data_file_path}")
        with open(data_file_path, 'w', encoding='utf-8') as f:
            json.dump(enriched_data, f, ensure_ascii=False, indent=4)

        logging.info("Arquivo de dados mestre enriquecido e salvo com sucesso!")

    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado: {e}", exc_info=True)


# --- BLOCO DE EXECU√á√ÉO ---
if __name__ == "__main__":
    if not MAIN_DATA_FILE.exists() or not CATALOGO_EMPRESAS_CSV.exists():
        logging.error("Erro: Um ou mais arquivos de entrada n√£o foram encontrados. Verifique os caminhos.")
    else:
        # Passo 1: Criar o cat√°logo de enriquecimento a partir do CSV
        enrichment_catalog = create_enrichment_catalog(CATALOGO_EMPRESAS_CSV)

        if enrichment_catalog:
            # Passo 2: Aplicar o cat√°logo para harmonizar e enriquecer, sobrescrevendo o arquivo principal
            enrich_and_overwrite_documents(MAIN_DATA_FILE, enrichment_catalog)


# FASE 1 (v6.0 - Otimiza√ß√£o com Data e Hierarquia)
#
# PASSO 3 (Final) do pipeline de processamento.
# Este script l√™ o arquivo de dados J√Å ENRIQUECIDO e gera os √≠ndices FAISS
# e os arquivos de chunks para o agente RAG, com metadados de data e hierarquia.

import json
import re
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import logging
import torch
from pathlib import Path
from collections import defaultdict

# --- Configura√ß√£o do Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

# --- Configura√ß√µes ---
PROJECT_DRIVE_PATH = Path('/content/drive/MyDrive/Projeto_CVM_RAG')
PROJECT_DRIVE_PATH.mkdir(parents=True, exist_ok=True)

INPUT_JSON_FILE = PROJECT_DRIVE_PATH / "cvm_documentos_texto_final.json"

# --- RECOMENDA√á√ÉO DE MODELO ---
# 'paraphrase-multilingual-mpnet-base-v2' √© um √≥timo ponto de partida.
# Para maior precis√£o em portugu√™s, considere testar 'neuralmind/bert-base-portuguese-cased'.
MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200
BATCH_SIZE = 32 # Ajuste conforme a mem√≥ria da sua GPU

# Assumindo que DICIONARIO_UNIFICADO_HIERARQUICO est√° carregado.

# --- FUN√á√ïES AUXILIARES (INALTERADAS) ---

def classify_document_by_url(url):
    """Classifica o tipo de documento com base em padr√µes na URL."""
    if 'frmExibirArquivoFRE' in url:
        return 'item_8_4'
    return 'outros_documentos'

def split_text_semantic(text, max_chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """Divide o texto em chunks, tentando respeitar os par√°grafos."""
    if not isinstance(text, str) or not text: return []
    paragraphs = text.split('\n\n')
    chunks, current_chunk = [], ""
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph: continue
        if len(current_chunk) + len(paragraph) + 1 <= max_chunk_size:
            current_chunk += paragraph + "\n\n"
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = paragraph + "\n\n"
    if current_chunk: chunks.append(current_chunk.strip())
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_chunk_size:
            start = 0
            while start < len(chunk):
                final_chunks.append(chunk[start:start + max_chunk_size])
                start += max_chunk_size - overlap
        else:
            final_chunks.append(chunk)
    return final_chunks

# --- FUN√á√ïES PARA ENRIQUECIMENTO (ADICIONADAS OU MODIFICADAS) ---

def _recursive_topic_finder(text, sub_dict, path_so_far, found_items):
    """Fun√ß√£o auxiliar recursiva para encontrar t√≥picos em dicion√°rios aninhados."""
    for topic_key, topic_data in sub_dict.items():
        current_path = path_so_far + [topic_key]
        # Ordena para garantir que aliases mais longos (e.g., "Op√ß√µes de Compra") sejam checados antes dos mais curtos ("Compra")
        sorted_aliases = sorted(topic_data.get("aliases", []), key=len, reverse=True)
        for alias in sorted_aliases:
            if re.search(r'\b' + re.escape(alias) + r'\b', text, re.IGNORECASE):
                found_items.add(tuple(current_path))
                break # Encontrou o t√≥pico, vai para o pr√≥ximo do dicion√°rio
        if "subtopicos" in topic_data and topic_data["subtopicos"]:
            _recursive_topic_finder(text, topic_data["subtopicos"], current_path, found_items)

def find_topics_recursive(text, dictionary_hierarchical):
    """Inicia a busca recursiva por t√≥picos hier√°rquicos em um texto."""
    if not isinstance(dictionary_hierarchical, dict): return []
    found_paths = set()
    for section, topics in dictionary_hierarchical.items():
        _recursive_topic_finder(text, topics, [section], found_paths)
    # Retorna os caminhos para serem processados depois
    return [list(path) for path in found_paths]

def extract_date_from_url(url: str) -> str:
    """Extrai a data de refer√™ncia da URL, se poss√≠vel."""
    match = re.search(r'data=(.+?)&', url)
    if match:
        return match.group(1).strip()
    return "N/A"

# --- FUN√á√ÉO PRINCIPAL (ATUALIZADA E OTIMIZADA) ---

def process_and_create_indices(all_docs, embedding_model, dictionary_to_use, model_name_str):
    """
    Orquestra a cria√ß√£o de chunks, embeddings e √≠ndices FAISS.
    (v6.0) - Enriquecimento com DATA DE REFER√äNCIA e HIERARQUIA DE T√ìPICOS.
    """
    categorized_docs = defaultdict(dict)
    for source_url, doc_data in all_docs.items():
        category = classify_document_by_url(source_url)
        if isinstance(doc_data, dict):
            categorized_docs[category][source_url] = doc_data

    logging.info("--- Iniciando cria√ß√£o de √≠ndices (v6.0 - Otimiza√ß√£o com Data e Hierarquia) ---")
    for category, docs in categorized_docs.items():
        logging.info(f"--- Processando categoria: '{category}' ({len(docs)} documentos) ---")
        all_chunks, chunk_map = [], []

        for source_url, doc_data in docs.items():
            doc_text = doc_data.get("text", "")
            company_name = doc_data.get("company_name", "N/A")
            setor = doc_data.get("setor", "N√£o Identificado")
            controle = doc_data.get("controle_acionario", "N√£o Identificado")
            document_date = doc_data.get("data_referencia", "N√£o Identificado")

            if not doc_text: continue

            doc_chunks = split_text_semantic(doc_text)
            for chunk_index, chunk in enumerate(doc_chunks):

                # --- Constru√ß√£o do Prefixo de Metadados Otimizado ---

                # 1. Metadados de Data e Categoria
                metadata_prefix_date = f"[data_documento:{document_date}]"
                metadata_prefix_cat = f"[empresa:{company_name}] [setor:{setor}] [controle:{controle}]"

                # 2. Metadados de T√≥picos (com hierarquia)
                topic_paths_as_lists = find_topics_recursive(chunk, dictionary_to_use)
                topic_paths_as_strings = ["/".join(path) for path in topic_paths_as_lists]
                topic_tags_list = []
                for path in topic_paths_as_lists:
                    for i, level_name in enumerate(path):
                        # ‚ú® OTIMIZA√á√ÉO 2: Criar tag para cada n√≠vel da hierarquia
                        topic_tags_list.append(f"[cat_nv{i+1}:{level_name}]")

                # Remove duplicatas mantendo a ordem (importante para hierarquia)
                unique_topic_tags = list(dict.fromkeys(topic_tags_list))
                metadata_prefix_topics = " ".join(unique_topic_tags)

                # 3. Combina√ß√£o para o Chunk Final
                full_prefix = f"{metadata_prefix_date} {metadata_prefix_cat} {metadata_prefix_topics}".strip()
                enriched_chunk = f"{full_prefix} {chunk}"
                all_chunks.append(enriched_chunk)

                # --- Adicionar metadados completos ao chunk_map ---
                chunk_map.append({
                    "company_name": company_name,
                    "source_url": source_url,
                    "chunk_id": f"{source_url}::{chunk_index}",
                    "document_date": document_date, # üîë Data adicionada para re-ranking
                    "setor": setor,
                    "controle_acionario": controle,
                    "topics_in_chunk": topic_paths_as_strings, # Mant√©m caminhos para an√°lise
                    "chunk_text": chunk # Armazena o texto original sem metadados
                })

        if not all_chunks:
            logging.warning(f"Nenhum chunk v√°lido foi gerado para a categoria '{category}'. Pulando.")
            continue

        logging.info(f"-> {len(all_chunks)} chunks √∫nicos criados para a categoria '{category}'.")
        logging.info("-> Gerando embeddings...")
        embeddings = embedding_model.encode(
            all_chunks,
            show_progress_bar=True,
            normalize_embeddings=True, # Normalizar √© crucial para busca com L2/Cosseno
            batch_size=BATCH_SIZE
        )
        embeddings = np.array(embeddings).astype('float32')
        dimension = embeddings.shape[1]

        logging.info("-> Criando √≠ndice FAISS...")
        # IndexFlatL2 √© um bom padr√£o. A dist√¢ncia L2 funciona bem com embeddings normalizados.
        index = faiss.IndexFlatL2(dimension)
        index_final = faiss.IndexIDMap(index)
        ids = np.array(range(len(all_chunks)))
        index_final.add_with_ids(embeddings, ids)
        logging.info(f"-> √çndice criado com {index_final.ntotal} vetores.")

        faiss_file = PROJECT_DRIVE_PATH / f"{category}_faiss_index.bin"
        chunks_file = PROJECT_DRIVE_PATH / f"{category}_chunks_map.json"

        faiss.write_index(index_final, str(faiss_file))
        logging.info(f"-> √çndice salvo em '{faiss_file}'")

        # Salva o mapa de chunks, que agora √© a sua "base de dados" de metadados
        with open(str(chunks_file), 'w', encoding='utf-8') as f:
            # Note a altera√ß√£o aqui, salvando apenas o chunk_map diretamente
            json.dump(chunk_map, f, ensure_ascii=False, indent=4)
        logging.info(f"-> Mapa de chunks salvo em '{chunks_file}'")


# --- BLOCO DE EXECU√á√ÉO (CORRIGIDO E COMPLETO) ---
if __name__ == "__main__":
    try:
        # --- CONFIGURA√á√ïES PRINCIPAIS ---
        MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
        PROJECT_DRIVE_PATH = Path('/content/drive/MyDrive/Projeto_CVM_RAG')
        PROJECT_DRIVE_PATH.mkdir(parents=True, exist_ok=True)
        INPUT_JSON_FILE = PROJECT_DRIVE_PATH / "cvm_documentos_texto_final.json"

        # --- DEFINA O SEU DICION√ÅRIO HIER√ÅRQUICO AQUI ---
        # Ele precisa ser acess√≠vel globalmente ou carregado.
        DICIONARIO_UNIFICADO_HIERARQUICO = {
            "Documentos Financeiros": {
                "Demonstrativos Financeiros": {
                    "aliases": ["DFC", "Demonstra√ß√£o do Fluxo de Caixa", "Balan√ßo Patrimonial", "BP"],
                    "subtopicos": {
                        "Ativo Circulante": {"aliases": ["Ativo circulante"]},
                        "Passivo Circulante": {"aliases": ["Passivo circulante"]}
                    }
                },
                "Relat√≥rio de Gest√£o": {
                    "aliases": ["Relat√≥rio da Administra√ß√£o", "Relat√≥rio de Gest√£o"],
                    "subtopicos": {}
                }
            },
            "Informa√ß√µes de Mercado": {
                "Governan√ßa Corporativa": {
                    "aliases": ["Governan√ßa Corporativa", "Conselho de Administra√ß√£o"],
                    "subtopicos": {}
                }
            }
        }
        # --- FIM DAS CONFIGURA√á√ïES ---

        # Configura√ß√£o de logging que n√£o interfere com a barra de progresso.
        logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

        if 'DICIONARIO_UNIFICADO_HIERARQUICO' not in globals():
            raise NameError("A vari√°vel 'DICIONARIO_UNIFICADO_HIERARQUICO' n√£o foi definida.")

        if not INPUT_JSON_FILE.exists():
            raise FileNotFoundError(f"Arquivo de entrada enriquecido n√£o encontrado: {INPUT_JSON_FILE}")

        logging.info(f"Lendo arquivo JSON enriquecido: {INPUT_JSON_FILE}")
        with open(INPUT_JSON_FILE, 'r', encoding='utf-8') as f:
            documents_data = json.load(f)

        device = "cuda" if torch.cuda.is_available() else "cpu"
        logging.info(f"Usando dispositivo: {device} para gera√ß√£o de embeddings.")

        logging.info(f"Carregando modelo de embedding '{MODEL_NAME}'...")
        model = SentenceTransformer(MODEL_NAME, device=device)

        # Chamada correta da fun√ß√£o, passando MODEL_NAME como o 4¬∫ argumento.
        process_and_create_indices(documents_data, model, DICIONARIO_UNIFICADO_HIERARQUICO, MODEL_NAME)

        logging.info(f"--- Processamento de √≠ndices com ¬†conclu√≠do com sucesso! ---")

    except (FileNotFoundError, NameError) as e:
        logging.error(f"ERRO DE CONFIGURA√á√ÉO OU ARQUIVO: {e}")
    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado: {e}", exc_info=True)

# extracao_logica_unificada.py (Vers√£o Otimizada)
#
# DESCRI√á√ÉO:
# Vers√£o ajustada e otimizada da l√≥gica de extra√ß√£o de t√≥picos.
# A altera√ß√£o principal corrige um gargalo de performance ao garantir que a
# limpeza do texto de entrada seja feita apenas uma vez, antes do in√≠cio da
# busca recursiva. A l√≥gica de detec√ß√£o e os resultados permanecem id√™nticos,
# mas a execu√ß√£o √© significativamente mais r√°pida e robusta.

import re
import unicodedata
from typing import List, Set, Tuple, Dict, Any

def normalizar_texto(texto: str) -> str:
    """
    (T√©cnica 1) Converte para min√∫sculas e remove acentos para uma compara√ß√£o robusta.
    """
    if not isinstance(texto, str):
        return ""
    # Converte para min√∫sculas
    texto = texto.lower()
    # Remove acentos (decomposi√ß√£o NFD)
    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')

# --- FUN√á√ÉO RECURSIVA OTIMIZADA ---
def _recursive_topic_finder_unificada(
    texto_limpo_para_busca: str,  # AJUSTE 1: Recebe o texto j√° limpo
    sub_dict: Dict[str, Any],
    path_so_far: List[str],
    found_items: Set[Tuple[Tuple[str, ...], str]]
):
    """
    Fun√ß√£o auxiliar recursiva que navega pelo dicion√°rio.
    Esta vers√£o √© otimizada para n√£o reprocessar o texto.
    """
    # A linha que limpava o texto foi REMOVIDA daqui, evitando reprocessamento.

    for topic_key, topic_data in sub_dict.items():
        current_path = path_so_far + [topic_key]

        # Ordena os aliases do maior para o menor para evitar correspond√™ncias parciais
        sorted_aliases = sorted(topic_data.get("aliases", []), key=len, reverse=True)

        for alias in sorted_aliases:
            alias_normalizado = normalizar_texto(alias)

            # A busca continua usando o texto limpo e a l√≥gica de palavra inteira (\b)
            if re.search(r'\b' + re.escape(alias_normalizado) + r'\b', texto_limpo_para_busca):
                found_items.add((tuple(current_path), alias))

        if "subtopicos" in topic_data and topic_data["subtopicos"]:
            # AJUSTE 2: Passa o mesmo texto j√° limpo para a pr√≥xima chamada recursiva
            _recursive_topic_finder_unificada(texto_limpo_para_busca, topic_data["subtopicos"], current_path, found_items)

# --- FUN√á√ÉO PRINCIPAL OTIMIZADA ---
def encontrar_topicos_unificado(texto_original: str, dicionario_hierarquico: Dict) -> List[Tuple[Tuple[str, ...], str]]:
    """
    Fun√ß√£o principal e unificada para encontrar t√≥picos hier√°rquicos e seus aliases.
    Esta √© a fun√ß√£o que seus outros scripts devem chamar.
    """
    if not isinstance(texto_original, str) or not isinstance(dicionario_hierarquico, dict):
        return []

    # --- OTIMIZA√á√ÉO APLICADA AQUI ---
    # Etapa 1: Normaliza√ß√£o (min√∫sculas, sem acentos)
    texto_normalizado = normalizar_texto(texto_original)
    # Etapa 2: Limpeza de pontua√ß√£o (feita apenas UMA VEZ para todo o texto)
    # Isso resolve o problema de encontrar termos como "(ROIC)" ou "TSR)".
    texto_limpo_para_busca = re.sub(r'[^\w\s]', ' ', texto_normalizado)

    found_items = set()
    # Itera sobre as se√ß√µes do dicion√°rio (ex: "TiposDePlano", "IndicadoresPerformance")
    for section, topics in dicionario_hierarquico.items():
        # AJUSTE 3: Passa o texto j√° limpo e pr√©-processado para a busca recursiva
        _recursive_topic_finder_unificada(texto_limpo_para_busca, topics, [section], found_items)

    return list(found_items)

# gerar_faiss_e_chunks_v7.py
#
# PASSO 3 (v7.1 - Vers√£o para Colab com Dicion√°rio Externo)
# Este script l√™ o arquivo de dados ENRIQUECIDO, gera os √≠ndices FAISS
# e os arquivos de chunks para o agente RAG, utilizando uma l√≥gica de
# extra√ß√£o de t√≥picos centralizada e aprimorada.

import json
import re
import numpy as np
import faiss
from sentence_transformers import SentenceTransformer
import logging
import torch
from pathlib import Path
from collections import defaultdict

# A l√≥gica unificada √© importada da c√©lula/arquivo executado anteriormente
# from extracao_logica_unificada import encontrar_topicos_unificado

# --- Configura√ß√£o do Logging ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

# --- Configura√ß√µes do Projeto ---
PROJECT_DRIVE_PATH = Path('/content/drive/MyDrive/Projeto_CVM_RAG')
PROJECT_DRIVE_PATH.mkdir(parents=True, exist_ok=True)

INPUT_JSON_FILE = PROJECT_DRIVE_PATH / "cvm_documentos_texto_final.json"
MODEL_NAME = 'sentence-transformers/paraphrase-multilingual-mpnet-base-v2'
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200
BATCH_SIZE = 32 # Ajuste conforme a mem√≥ria da sua GPU

# --- FUN√á√ïES AUXILIARES ---

def classify_document_by_url(url):
    """Classifica o tipo de documento com base em padr√µes na URL."""
    if 'frmExibirArquivoFRE' in url:
        return 'item_8_4'
    return 'outros_documentos'

def split_text_semantic(text, max_chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    """Divide o texto em chunks, tentando respeitar os par√°grafos."""
    if not isinstance(text, str) or not text: return []
    paragraphs = text.split('\n\n')
    chunks, current_chunk = [], ""
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph: continue
        if len(current_chunk) + len(paragraph) + 1 <= max_chunk_size:
            current_chunk += paragraph + "\n\n"
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = paragraph + "\n\n"
    if current_chunk: chunks.append(current_chunk.strip())

    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_chunk_size:
            start = 0
            while start < len(chunk):
                final_chunks.append(chunk[start:start + max_chunk_size])
                start += max_chunk_size - overlap
        else:
            final_chunks.append(chunk)
    return final_chunks

# --- FUN√á√ÉO PRINCIPAL ---

def process_and_create_indices(all_docs, embedding_model, dictionary_to_use):
    """
    Orquestra a cria√ß√£o de chunks, embeddings e √≠ndices FAISS.
    """
    categorized_docs = defaultdict(dict)
    for source_url, doc_data in all_docs.items():
        category = classify_document_by_url(source_url)
        if isinstance(doc_data, dict):
            categorized_docs[category][source_url] = doc_data

    logging.info("--- Iniciando cria√ß√£o de √≠ndices (v7.1 - L√≥gica Unificada) ---")
    for category, docs in categorized_docs.items():
        logging.info(f"--- Processando categoria: '{category}' ({len(docs)} documentos) ---")
        all_chunks, chunk_map = [], []

        for source_url, doc_data in docs.items():
            doc_text = doc_data.get("text", "")
            company_name = doc_data.get("company_name", "N/A")
            setor = doc_data.get("setor", "N√£o Identificado")
            controle = doc_data.get("controle_acionario", "N√£o Identificado")
            document_date = doc_data.get("data_referencia", "N√£o Identificado")

            if not doc_text: continue

            doc_chunks = split_text_semantic(doc_text)
            for chunk_index, chunk in enumerate(doc_chunks):

                # 1. Metadados de Data e Categoria
                metadata_prefix_date = f"[data_documento:{document_date}]"
                cat_parts = []
                if company_name and company_name != "N/A":
                    cat_parts.append(f"[empresa:{company_name}]")
                if setor and setor not in ["N√£o Identificado", "NaN"]:
                    cat_parts.append(f"[setor:{setor}]")
                if controle and controle not in ["N√£o Identificado", "NaN"]:
                    cat_parts.append(f"[controle:{controle}]")
                metadata_prefix_cat = " ".join(cat_parts)

                # 2. Metadados de T√≥picos (com hierarquia) via L√ìGICA UNIFICADA
                topic_path_alias_tuples = encontrar_topicos_unificado(chunk, dictionary_to_use)
                topic_paths_as_lists = [list(path) for path, alias in topic_path_alias_tuples]
                topic_paths_as_strings = sorted(list(set(["/".join(path) for path in topic_paths_as_lists])))

                topic_tags_list = []
                for path in topic_paths_as_lists:
                    for i, level_name in enumerate(path):
                        topic_tags_list.append(f"[cat_nv{i+1}:{level_name}]")

                unique_topic_tags = sorted(list(dict.fromkeys(topic_tags_list)))
                metadata_prefix_topics = " ".join(unique_topic_tags)

                # 3. Combina√ß√£o para o Chunk Final
                full_prefix = f"{metadata_prefix_date} {metadata_prefix_cat} {metadata_prefix_topics}".strip()
                enriched_chunk = f"{full_prefix} {chunk}"
                all_chunks.append(enriched_chunk)

                # 4. Adicionar metadados completos ao chunk_map
                chunk_map.append({
                    "company_name": company_name,
                    "source_url": source_url,
                    "chunk_id": f"{source_url}::{chunk_index}",
                    "document_date": document_date,
                    "setor": setor,
                    "controle_acionario": controle,
                    "topics_in_chunk": topic_paths_as_strings,
                    "chunk_text": chunk
                })

        if not all_chunks:
            logging.warning(f"Nenhum chunk v√°lido foi gerado para a categoria '{category}'. Pulando.")
            continue

        logging.info(f"-> {len(all_chunks)} chunks √∫nicos criados para a categoria '{category}'.")
        logging.info("-> Gerando embeddings...")
        embeddings = embedding_model.encode(
            all_chunks, show_progress_bar=True, normalize_embeddings=True, batch_size=BATCH_SIZE
        )
        embeddings = np.array(embeddings).astype('float32')
        dimension = embeddings.shape[1]

        logging.info("-> Criando √≠ndice FAISS...")
        index = faiss.IndexFlatL2(dimension)
        index_final = faiss.IndexIDMap(index)
        ids = np.array(range(len(all_chunks)))
        index_final.add_with_ids(embeddings, ids)
        logging.info(f"-> √çndice criado com {index_final.ntotal} vetores.")

        faiss_file = PROJECT_DRIVE_PATH / f"{category}_faiss_index.bin"
        chunks_file = PROJECT_DRIVE_PATH / f"{category}_chunks_map.json"

        faiss.write_index(index_final, str(faiss_file))
        logging.info(f"-> √çndice salvo em '{faiss_file}'")

        with open(str(chunks_file), 'w', encoding='utf-8') as f:
            json.dump(chunk_map, f, ensure_ascii=False, indent=4)
        logging.info(f"-> Mapa de chunks salvo em '{chunks_file}'")


# --- BLOCO DE EXECU√á√ÉO ---
if __name__ == "__main__":
    try:
        # Verifica se o dicion√°rio foi definido em uma c√©lula anterior
        if 'DICIONARIO_UNIFICADO_HIERARQUICO' not in globals() or not DICIONARIO_UNIFICADO_HIERARQUICO:
            raise NameError("A vari√°vel 'DICIONARIO_UNIFICADO_HIERARQUICO' n√£o foi definida ou est√° vazia. Execute a c√©lula do dicion√°rio primeiro.")
        if not INPUT_JSON_FILE.exists():
            raise FileNotFoundError(f"Arquivo de entrada enriquecido n√£o encontrado: {INPUT_JSON_FILE}")

        logging.info(f"Lendo arquivo JSON enriquecido: {INPUT_JSON_FILE}")
        with open(INPUT_JSON_FILE, 'r', encoding='utf-8') as f:
            documents_data = json.load(f)

        device = "cuda" if torch.cuda.is_available() else "cpu"
        logging.info(f"Usando dispositivo: {device} para gera√ß√£o de embeddings.")

        logging.info(f"Carregando modelo de embedding '{MODEL_NAME}'...")
        model = SentenceTransformer(MODEL_NAME, device=device)

        process_and_create_indices(documents_data, model, DICIONARIO_UNIFICADO_HIERARQUICO)

        logging.info(f"--- Processamento de √≠ndices conclu√≠do com sucesso! ---")

    except (FileNotFoundError, NameError) as e:
        logging.error(f"ERRO DE CONFIGURA√á√ÉO OU ARQUIVO: {e}")
    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado: {e}", exc_info=True)

# gerar_resumo_v7.4_extracao_robusta.py
#
# DESCRI√á√ÉO:
# Vers√£o baseada na v7.3 otimizada, com foco em garantir a robustez da extra√ß√£o
# de fatos (TSR, ROIC, etc.), resolvendo especificamente o problema de
# pontua√ß√µes adjacentes √†s palavras-chave.

import json
import re
import logging
from pathlib import Path
import unicodedata
from typing import List, Set, Tuple, Dict, Any
from collections import defaultdict

# --- Configura√ß√µes ---
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s', force=True)

PROJECT_DRIVE_PATH = Path('/content/drive/MyDrive/Projeto_CVM_RAG')
INPUT_JSON_FILE = PROJECT_DRIVE_PATH / "cvm_documentos_texto_final.json"
# Atualizado o nome do arquivo de sa√≠da
OUTPUT_SUMMARY_FILE = PROJECT_DRIVE_PATH / "resumo_planos_granulares_v7.4.json"
CHUNK_SIZE = 1500
CHUNK_OVERLAP = 200

# --- DICION√ÅRIO ---
# O script assume que a vari√°vel 'DICIONARIO_UNIFICADO_HIERARQUICO'
# j√° foi definida em uma c√©lula anterior no ambiente do Colab.


# ==============================================================================
# OTIMIZA√á√ÉO: PR√â-COMPILA√á√ÉO DAS EXPRESS√ïES REGULARES
# ==============================================================================
def _compile_fact_regexes() -> Dict[str, re.Pattern]:
    """Pr√©-compila regex para serem reutilizadas, melhorando a performance."""
    logging.info("Compilando express√µes regulares para extra√ß√£o de fatos...")
    num_pattern = r'(\d{1,3}(?:[.,]\d{3})*|\d+|um|uma|dois|duas|tres|tr√™s|quatro|cinco|seis|sete|oito|nove|dez)'
    unit_pattern_anomesdias = r'\b(anos?|meses|dias)\b'

    return {
        'vesting': re.compile(
            fr'(?:vesting|periodo\s+de\s+carencia|prazo\s+de\s+carencia|periodo\s+de\s+aquisicao|prazo\s+de\s+aquisicao)'
            fr'[\s\S]{{0,250}}?(?:de\s+)?{num_pattern}\s*\(?[\s\S]{{0,100}}?{unit_pattern_anomesdias}', re.IGNORECASE
        ),
        'lockup': re.compile(
            fr'(?:lock-up|periodo\s+de\s+restricao|restricao\s+a\s+venda)'
            fr'[\s\S]{{0,250}}?(?:de\s+)?{num_pattern}\s*\(?[\s\S]{{0,100}}?{unit_pattern_anomesdias}', re.IGNORECASE
        ),
        'diluicao': re.compile(
            r'(?:diluicao|limite|nao\s+exceda|representativas\s+de|no\s+maximo)'
            r'[\s\S]{0,250}?\b(\d{1,3}(?:[.,]\d{1,2})?)\s*%', re.IGNORECASE
        ),
        'desconto': re.compile(
            r'(?:desconto|desagio|abatimento|reducao)[\s\S]{0,100}?\b(\d{1,3}(?:[.,]\d{1,2})?)\s*%', re.IGNORECASE
        ),
        'malus_clawback': re.compile(r'\b(malus|clawback|clausula\s+de\s+recuperacao|forfeiture|nao\s+concorrencia)\b', re.IGNORECASE),
        'dividendos': re.compile(r'(?:dividendos|jcp|juros\s+sobre\s+capital\s+proprio|proventos)[\s\S]{0,200}?(?:durante|no\s+periodo\s+de)\s*(?:carencia|vesting)', re.IGNORECASE),
        # Padr√µes para TSR e ROIC, que buscam a frase completa ou a sigla.
        'tsr': re.compile(r'\b(retorno\s+total\s+d[oa]s\s+acionistas|total\s+shareholder\s+return|tsr)\b', re.IGNORECASE),
        'roic': re.compile(r'\b(retorno\s+sobre\s+o\s+capital\s+investido|return\s+on\s+invested\s+capital|roic)\b', re.IGNORECASE),
    }

REGEX_FATOS_COMPILADOS = _compile_fact_regexes()


# ==============================================================================
# FUN√á√ïES AUXILIARES (L√ìGICA ORIGINAL v7 com OTIMIZA√á√ïES)
# ==============================================================================

def split_text_semantic(text, max_chunk_size=CHUNK_SIZE, overlap=CHUNK_OVERLAP):
    if not isinstance(text, str) or not text: return []
    paragraphs = text.split('\n\n')
    chunks, current_chunk = [], ""
    for paragraph in paragraphs:
        paragraph = paragraph.strip()
        if not paragraph: continue
        if len(current_chunk) + len(paragraph) + 1 <= max_chunk_size:
            current_chunk += paragraph + "\n\n"
        else:
            if current_chunk: chunks.append(current_chunk.strip())
            current_chunk = paragraph + "\n\n"
    if current_chunk: chunks.append(current_chunk.strip())
    final_chunks = []
    for chunk in chunks:
        if len(chunk) > max_chunk_size:
            start = 0
            while start < len(chunk):
                final_chunks.append(chunk[start:start + max_chunk_size])
                start += max_chunk_size - overlap
        else:
            final_chunks.append(chunk)
    return final_chunks

def normalizar_texto(texto: str) -> str:
    if not isinstance(texto, str): return ""
    texto = texto.lower()
    return ''.join(c for c in unicodedata.normalize('NFD', texto) if unicodedata.category(c) != 'Mn')

def _recursive_topic_finder_unificada(
    texto_limpo_para_busca: str,
    sub_dict: Dict[str, Any],
    path_so_far: List[str],
    found_items: Set[Tuple[Tuple[str, ...], str]]
):
    for topic_key, topic_data in sub_dict.items():
        current_path = path_so_far + [topic_key]
        sorted_aliases = sorted(topic_data.get("aliases", []), key=len, reverse=True)
        for alias in sorted_aliases:
            alias_normalizado = normalizar_texto(alias)
            if re.search(r'\b' + re.escape(alias_normalizado) + r'\b', texto_limpo_para_busca):
                found_items.add((tuple(current_path), alias))
        if "subtopicos" in topic_data and topic_data["subtopicos"]:
            _recursive_topic_finder_unificada(texto_limpo_para_busca, topic_data["subtopicos"], current_path, found_items)

def encontrar_topicos_unificado(texto_original: str, dicionario_hierarquico: Dict) -> List[Tuple[Tuple, str]]:
    if not isinstance(texto_original, str) or not isinstance(dicionario_hierarquico, dict): return []
    texto_normalizado = normalizar_texto(texto_original)
    texto_limpo_para_busca = re.sub(r'[^\w\s]', ' ', texto_normalizado)
    found_items = set()
    for section, topics in dicionario_hierarquico.items():
        _recursive_topic_finder_unificada(texto_limpo_para_busca, topics, [section], found_items)
    return list(found_items)

def _converter_palavra_para_int(palavra: str) -> int:
    if not isinstance(palavra, str): return 0
    palavra_limpa = normalizar_texto(palavra).strip()
    mapeamento = {'um': 1, 'uma': 1, 'dois': 2, 'duas': 2, 'tres': 3, 'quatro': 4, 'cinco': 5, 'seis': 6, 'sete': 7, 'oito': 8, 'nove': 9, 'dez': 10}
    if palavra_limpa in mapeamento: return mapeamento[palavra_limpa]
    try:
        return int(float(palavra_limpa.replace(',', '.')))
    except (ValueError, TypeError):
        return 0

def build_nested_dict_from_paths(path_alias_tuples):
    nested_dict = {}
    for path, alias in path_alias_tuples:
        d = nested_dict
        for key in path[:-1]: d = d.setdefault(key, {})
        leaf_key = path[-1]
        leaf_node = d.setdefault(leaf_key, {})
        leaf_node.setdefault('_aliases', set()).add(alias)
    def cleanup_dict(d):
        result = {}
        for k, v in d.items():
            sub_dict = {sk: sv for sk, sv in v.items() if sk != '_aliases'}
            aliases = sorted(list(v.get('_aliases', set())))
            cleaned_sub = cleanup_dict(sub_dict)
            if cleaned_sub and aliases: result[k] = {"_aliases": aliases, **cleaned_sub}
            elif cleaned_sub: result[k] = cleaned_sub
            elif aliases: result[k] = aliases
        return result
    return cleanup_dict(nested_dict)

# --- FUN√á√ÉO DE EXTRA√á√ÉO DE FATOS ATUALIZADA ---
def extract_structured_facts(text: str) -> dict:
    """
    Extrai fatos estruturados de um texto, usando texto pr√©-processado para
    garantir a detec√ß√£o correta mesmo com pontua√ß√£o.
    """
    facts = {}
    # Etapa 1: Normaliza√ß√£o b√°sica (min√∫sculas, sem acentos)
    text_normalized = normalizar_texto(text)
    # Etapa 2: Limpeza de pontua√ß√£o para busca robusta de palavras-chave
    text_limpo_para_busca = re.sub(r'[^\w\s]', ' ', text_normalized)

    # Extra√ß√£o de fatos quantitativos (usam o texto normalizado para preservar a estrutura)
    try:
        if match := REGEX_FATOS_COMPILADOS['vesting'].search(text_normalized):
            valor_num = _converter_palavra_para_int(match.group(1))
            unidade = match.group(2).lower().rstrip('s')
            valor_em_anos = valor_num if unidade == 'ano' else valor_num / 12.0 if unidade == 'mes' else valor_num / 365.0
            if 0 < valor_em_anos < 20: facts['periodo_vesting'] = {'presente': True, 'valor': round(valor_em_anos, 2), 'unidade': 'ano'}
    except Exception as e: logging.warning(f"Alerta na extra√ß√£o de Fato (Vesting): {e}")

    try:
        if match := REGEX_FATOS_COMPILADOS['lockup'].search(text_normalized):
            valor_num = _converter_palavra_para_int(match.group(1))
            unidade = match.group(2).lower().rstrip('s')
            valor_em_anos = valor_num if unidade == 'ano' else valor_num / 12.0 if unidade == 'mes' else valor_num / 365.0
            if 0 < valor_em_anos < 20: facts['periodo_lockup'] = {'presente': True, 'valor': round(valor_em_anos, 2), 'unidade': 'ano'}
    except Exception as e: logging.warning(f"Alerta na extra√ß√£o de Fato (Lock-up): {e}")

    try:
        if match := REGEX_FATOS_COMPILADOS['diluicao'].search(text_normalized):
            valor_percentual = float(match.group(1).replace(',', '.')) / 100.0
            if 0 < valor_percentual < 0.5: facts['diluicao_maxima_percentual'] = {'presente': True, 'valor': valor_percentual, 'unidade': 'percentual'}
    except Exception as e: logging.warning(f"Alerta na extra√ß√£o de Fato (Dilui√ß√£o Percentual): {e}")

    try:
        if match := REGEX_FATOS_COMPILADOS['desconto'].search(text_normalized):
            valor_str = match.group(1).replace(',', '.')
            facts['desconto_strike_price'] = {'presente': True, 'valor_numerico': float(valor_str) / 100, 'tipo': 'expl√≠cito'}
    except Exception as e: logging.warning(f"Alerta na extra√ß√£o de Fato (Desconto): {e}")

    # --- L√ìGICA DE BUSCA ROBUSTA APLICADA AQUI ---
    # Fatos de presen√ßa/aus√™ncia usam o texto limpo, resolvendo o problema da pontua√ß√£o.
    if REGEX_FATOS_COMPILADOS['malus_clawback'].search(text_limpo_para_busca): facts['malus_clawback_presente'] = {'presente': True}
    if REGEX_FATOS_COMPILADOS['dividendos'].search(text_limpo_para_busca): facts['dividendos_durante_carencia'] = {'presente': True}
    if REGEX_FATOS_COMPILADOS['tsr'].search(text_limpo_para_busca): facts['t_s_r_presente'] = {'presente': True}
    if REGEX_FATOS_COMPILADOS['roic'].search(text_limpo_para_busca): facts['r_o_i_c_presente'] = {'presente': True}

    return facts

# ==============================================================================
# FUN√á√ÉO PRINCIPAL (L√ìGICA ORIGINAL v7 INTACTA)
# ==============================================================================
def generate_granular_summary_file(all_docs, dictionary_to_use):
    logging.info("--- Iniciando gera√ß√£o de resumo granular (v7.4 - Extra√ß√£o Robusta) ---")

    empresas_features = {}
    tipos_de_plano_dict = {"TiposDePlano": dictionary_to_use.get("TiposDePlano", {})}

    for i, (source_url, doc_data) in enumerate(all_docs.items()):
        company_name = doc_data.get("company_name")
        doc_text = doc_data.get("text", "")

        logging.info(f"Processando documento {i+1}/{len(all_docs)}: {company_name}")
        if not company_name or not doc_text: continue

        if company_name not in empresas_features:
            empresas_features[company_name] = {
                "setor": doc_data.get("setor", "N√£o Identificado"),
                "controle_acionario": doc_data.get("controle_acionario", "N√£o Identificado"),
                "planos_identificados": {},
                "fatos_extraidos_agregados": {}
            }

        found_plans_tuples = encontrar_topicos_unificado(doc_text, tipos_de_plano_dict)
        identified_plan_names = {path[-1] for path, alias in found_plans_tuples}
        if not identified_plan_names:
            identified_plan_names = {"PlanoGeralNaoIdentificado"}

        doc_all_topics = encontrar_topicos_unificado(doc_text, dictionary_to_use)

        doc_chunks = split_text_semantic(doc_text, CHUNK_SIZE, CHUNK_OVERLAP)
        empresas_features[company_name]["fatos_extraidos_agregados"] = {}
        for chunk in doc_chunks:
            # A chamada agora usa a fun√ß√£o de extra√ß√£o atualizada
            chunk_facts = extract_structured_facts(chunk)
            if chunk_facts:
                empresas_features[company_name]["fatos_extraidos_agregados"].update(chunk_facts)

        for plan_name in identified_plan_names:
            plan_entry = empresas_features[company_name]["planos_identificados"].setdefault(
                plan_name,
                { "documentos_fonte": set(), "topicos_encontrados_com_alias": set(), "fatos_extraidos": {} }
            )
            plan_entry["documentos_fonte"].add(source_url)
            plan_entry["topicos_encontrados_com_alias"].update(doc_all_topics)
            plan_entry["fatos_extraidos"].update(empresas_features[company_name]["fatos_extraidos_agregados"])

    logging.info("Finalizando e formatando o JSON de sa√≠da...")
    final_output = {}
    for company, data in empresas_features.items():
        final_output[company] = {
            "setor": data["setor"],
            "controle_acionario": data["controle_acionario"],
            "fatos_extraidos": data["fatos_extraidos_agregados"],
            "planos_identificados": {}
        }
        for plan_name, plan_data in data["planos_identificados"].items():
            nested_topics = build_nested_dict_from_paths(plan_data["topicos_encontrados_com_alias"])
            final_output[company]["planos_identificados"][plan_name] = {
                "documentos_fonte": sorted(list(plan_data["documentos_fonte"])),
                "topicos_encontrados": nested_topics,
                "fatos_extraidos": plan_data["fatos_extraidos"]
            }

    with open(OUTPUT_SUMMARY_FILE, 'w', encoding='utf-8') as f:
        json.dump(final_output, f, ensure_ascii=False, indent=4)

    logging.info(f"‚úÖ Resumo granular salvo com sucesso em '{OUTPUT_SUMMARY_FILE}'")

# ==============================================================================
# BLOCO DE EXECU√á√ÉO
# ==============================================================================
def main():
    try:
        if 'DICIONARIO_UNIFICADO_HIERARQUICO' not in globals() or not DICIONARIO_UNIFICADO_HIERARQUICO:
            raise NameError("A vari√°vel 'DICIONARIO_UNIFICADO_HIERARQUICO' n√£o foi definida ou est√° vazia. Execute a c√©lula do dicion√°rio primeiro.")
        if not INPUT_JSON_FILE.exists():
            raise FileNotFoundError(f"Arquivo de entrada n√£o encontrado: {INPUT_JSON_FILE}")

        logging.info(f"Lendo arquivo JSON enriquecido: {INPUT_JSON_FILE}")
        with open(INPUT_JSON_FILE, 'r', encoding='utf-8') as f:
            documents_data = json.load(f)

        generate_granular_summary_file(documents_data, DICIONARIO_UNIFICADO_HIERARQUICO)
    except (FileNotFoundError, NameError) as e:
        logging.error(f"ERRO DE CONFIGURA√á√ÉO OU ARQUIVO: {e}")
    except Exception as e:
        logging.error(f"Ocorreu um erro inesperado: {e}", exc_info=True)

# Para rodar, chame a fun√ß√£o main() em uma c√©lula do notebook ap√≥s definir o dicion√°rio.
main()